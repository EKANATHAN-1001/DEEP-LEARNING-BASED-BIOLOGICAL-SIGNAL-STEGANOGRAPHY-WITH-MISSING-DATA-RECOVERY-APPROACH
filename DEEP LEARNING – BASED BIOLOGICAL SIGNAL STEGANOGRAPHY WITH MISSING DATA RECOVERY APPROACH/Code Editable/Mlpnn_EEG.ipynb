{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gHnjoALH-rE",
        "outputId": "f9996b19-6b78-45a9-822c-0444f954d807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyswarm in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyswarm) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "pip install pyswarm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ6sJNdIHtPc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pyswarm import pso\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB28IowTHvEo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/EEG_Data.csv')\n",
        "x_data = df['Time'][:200].values\n",
        "y_data = df['Data'][:200].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "vUtezq7VIsxE",
        "outputId": "aebbe860-d5ac-4d77-e3e0-f50ae4a46b57"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGfCAYAAABBU+jJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACExElEQVR4nO39eZxU9Z39j5/aq/em6Q2EhhYVRBAVFdEJbgRQkmg0m5pEo9HRaU3AxDAkxqiZCYmZ6C+TOJpMomQmGo2/j8ZEMyoqoCi4oChLRPZF6Aaa3pda7/ePqvddqm5V3Vt1by30eT4e/dCuun37VhdwT5/Xeb1eDkmSJBBCCCGElBDOQl8AIYQQQohZKGAIIYQQUnJQwBBCCCGk5KCAIYQQQkjJQQFDCCGEkJKDAoYQQgghJQcFDCGEEEJKDgoYQgghhJQcFDCEEEIIKTkoYAghhBBScrjNHLxs2TI8/fTT+Oijj1BWVoZzzz0XP/vZzzB58mQAwNGjR/GjH/0IL730Evbu3YuGhgZcfvnl+PGPf4yamhr5PHv37sUtt9yClStXorKyEtdeey2WLVsGt1u5nFWrVuH222/H5s2bMX78eNx555247rrrDF9rNBrFgQMHUFVVBYfDYeZlEkIIIaRASJKEvr4+jB07Fk5nGp9FMsH8+fOlRx99VNq0aZO0YcMG6dJLL5VaWlqk/v5+SZIkaePGjdIVV1wh/fWvf5W2b98uvfLKK9KJJ54oXXnllfI5wuGwNG3aNGnu3LnS+++/L/3973+X6uvrpaVLl8rH7Ny5UyovL5duv/12acuWLdKvfvUryeVySS+88ILha923b58EgB/84Ac/+MEPfpTgx759+9Le5x25LHM8fPgwGhsbsXr1asyZM0f3mKeeegpf/epXMTAwALfbjf/7v//DZz7zGRw4cABNTU0AgIcffhhLlizB4cOH4fV6sWTJEjz//PPYtGmTfJ6vfOUr6O7uxgsvvGDo2np6elBbW4t9+/ahuro625dICCGEkDzS29uL8ePHo7u7W1O9ScRUCSmRnp4eAEBdXV3aY6qrq+Xy0Nq1azF9+nRZvADA/Pnzccstt2Dz5s04/fTTsXbtWsydO1dznvnz52PRokUpv08gEEAgEJA/7+vrAwBUV1dTwBBCCCElRqb4R9Yh3mg0ikWLFuG8887DtGnTdI85cuQIfvzjH+Omm26SH2tvb9eIFwDy5+3t7WmP6e3txdDQkO73WrZsGWpqauSP8ePHZ/vSCCGEEFLkZC1g2trasGnTJjzxxBO6z/f29mLhwoWYOnUq7r777my/jWGWLl2Knp4e+WPfvn22f09CCCGEFIasSki33nornnvuObz22msYN25c0vN9fX1YsGABqqqq8Mwzz8Dj8cjPNTc34+2339Yc39HRIT8n/iseUx9TXV2NsrIy3Wvy+Xzw+XzZvBxCCCGElBimHBhJknDrrbfimWeewauvvorW1takY3p7ezFv3jx4vV789a9/hd/v1zw/e/ZsbNy4EYcOHZIfW7FiBaqrqzF16lT5mFdeeUXzdStWrMDs2bPNXC4hhBBCjlFMCZi2tjb88Y9/xOOPP46qqiq0t7ejvb1dzqUI8TIwMIDf//736O3tlY+JRCIAgHnz5mHq1Kn42te+hg8++AAvvvgi7rzzTrS1tckOys0334ydO3fie9/7Hj766CP813/9F/785z9j8eLFFr98QgghhJQiptqoUyWCH330UVx33XVYtWoVLrzwQt1jdu3ahYkTJwIA9uzZg1tuuQWrVq1CRUUFrr32Wvz0pz9NGmS3ePFibNmyBePGjcMPf/hDU4Psent7UVNTI3dBEUIIIaT4MXr/zmkOTDFDAUMIIYSUHkbv39yFRAghhJCSgwKGEEIIISUHBQwhhBBCSg4KGEIIIYSUHBQwhBBCCCk5clrmSApPNCrhkTd24ZPu2CyeM1pG4bMzxhb4qgghhBB7oYApcdbv7cK/Pf8P+fP/WbsH509uQLXfk+artHQPBtEfCGseG13hQ5nXZdl1EkIIIVZCAVPiHB0IAgDG1PhxpD+AUERCz2DIsIB5fdthfP2Rt5E4Daja78bqOy7EqAqv1ZdMCCGE5AwzMCXOcCi2ouH4hgpU+tyax4zw36/vgiQBHpcDfo8Tfk/sj0TvcBg7jwxYf8GEEEKIBdCBKXGEWPG7XSj3utE1GMJg0JiA2d81iNe3HQYAvHz7+ZgwugIAMPf+1dh+qB+BsHEhRAghhOQTOjAlzlBcrPi9Ltk9MSpg/vzufkgScO6k0bJ4AQCvK3aeYDhq8dUSQggh1kABY5I/vLkbn/nV6/jd6zsLfSkAgOG4yBAODAAMhcLpvgQAEIlKeOrdfQCAr5zdonnO56GAIYQQUtywhGSSw30BbPqkF2dOqCv0pQBQHJgyr1PuGkrnwOzvGsS7u7uw68gADvYMo7bcg3lTmzTHCAcmQAFDCCGkSKGAMYko05gJytrJcDynUuZxodyAgPna79/GLlU494rTx8Hv0bZLe910YAghhBQ3FDAmETf7ohEwIgOjEjBDKQSMJEnYe3QQAHB2ax3qK724+fzjk47zuWPnCUYoYAghhBQnFDAm8cUFzFCxCJhQPAPjcaHMIzIw+tcWCEcRicYGvvz+2jNRlWJWjI8ODCGEkCKHIV6T+N2ihFQcN3chVvwGSkgDqmm7IvCrhyghsY2aEEJIsUIBY5JiKyEJAVOmKSHpdyEJYVPmccHldKQ8Jx0YQgghxQ4FjElkAVMkN3d5kJ3HKV9bKgdG7Duq8KWvHDLESwghpNihgDGJ6EIKFIkDM6zrwKQvIVX40i9pZBs1IYSQYocCxiRlRVZCkkO8XgMZmPjjFWnyL4AyyI4ChhBCSLFCAWMSJQNTHDf3IdUupLK4MBlMIa6MOzBsoyaEEFLcUMCYRB5kVyQdOnIJyZs5xDtgMgMTKBKRRgghhCRCAWMSMeQtVc4k36hDvGKVQKo5MEYFjNyFRAeGEEJIkUIBYxJRQgqEo5AkqcBXo9qF5HGhPEMXkpKByVBCkruQikOkEUIIIYlQwJhElJCAwodcJUlStlF7XIoDk7ELiW3UhBBCShsKGJOoFx8WuhMpFJHk1QBmJvFm7EJyswuJEEJIcUMBYxKPywl3fIptoTuR1EHiWAYmvgspUwnJaAaGAoYQQkiRQgGTBcWyTkBsonY6YsPnRAYmGIkirBPANdxGzRAvIYSQIocCJguKpZVaOEBlHhccDoecgQH0Z8EYHWQn5sCwjZoQQkixQgGTBcXSSq3eRA3ESj9iR+OwzrUZbqP20IEhhBBS3FDAZIHswBTYoUgUMA6HA+ViGm9aAWNsFxIzMIQQQooVCpgsUDZSF7qEpAyxE5Sl6UQaCJqcxMs5MIQQQooUCpgskIfZFTjEO6RaIyAQyyaHQsnrBAYCBpc5so2aEEJIkUMBkwXFUkIKqBY5CtLNgjHdhUQBQwghpEihgMmCsiJpo9Z1YFIImHAkKjsqGbuQVA5MMaxLIIQQQhKhgMkCX5EIGOEAqacDl6dYJzCg+jzzIDvlfKEIBQwhhJDigwImC0TJZrjAJRYhUtQCpsyj34Ukykdel1N2WFLhUz3PVmpCCCHFCAVMFogMTLHMgSlTdSHJDkxIX8CUZ8i/AEobNcAcDCGEkOKEAiYLiqWNOhBKdmCUEpK2C8noFF4AcDod8LhiE/HYSk0IIaQYoYDJAuHAFHrUvuLAKAJGiJlUJaRMHUgCDrMjhBBSzGT+dZwkIWdgiiTE69NxYFILGGNvudftxEAwcswLmLd2dmLN9iMAgNEVXlw1q0UTYiaEEFKcUMBkgWhVLrSA0XNgUnchxQRMpUEBE7uJh5KG2UWiEvqGQ5rHvG6nvMKglJAkCTf+z7voHVbKbQ1Vfiw8dUwBr4oQQogRSu+uUwQobdSFdSeGdUK8ZWIXUlKIN/Z5uddgCUlnGm84EsWl//k6Pu7o1xzrcAD3f2kGPn/6OJOvoLAcHQiidzgMhwMYN6oM+44OoWcolPkLCSGEFBxmYLLAH7+5FzrEO2wmxJtFCQnQZmAO9QWSxAsASBLw8KqdJTf07mDPMACgvtKHaWNrAACR6LFdMiOEkGMFOjBZIARD0bRRe3UETIo2aiNdSIB6H5JyHnGO2nIP1t/5aQBA71AI5yx7BVs7+vDB/h6cNr42i1dinr7hEPYeHdQ8VulzY8LoCsPnONA9BAAYW+OHyxnrugpHS0uEEULISIUCJguUNupCl5DiIV63epBdihCvaKPOwYHpDyg5GnHDH1XhxcLpY/D0+5/gibf35kXABMIRXPgfq3GkP5D03H1fOBVfOnO8ofO098YcmOYaPzzxrqsIBQwhhJQELCFlgdJGXWAHJqjnwLg1zwkUB8ZkG3VEX8Co+fJZMcHw1w8OyMfYSddASBYvjVU+NFb5UBW/prd2HjV8ngPdMQEzpqaMDgwhhJQYFDBZ4C+WXUhhsY1aHeKN/X+uDowIKqtn3QykEDBnt9bh+PoKDAYj+NWr2/Dylg7s79KWd6wkFBdV5V4X3v7BXLz9g7n4yRXTAQB7OgcMn+dgT7yEVOuHOy5g6MAQQkhpQAGTBcocmAKXkHQcmEy7kIy2Ues5MH3D+kFgh8MhuzC/Wb0T3/yfd3HpL1+3bYqvuCaPauVBa30s+7LbjIDpFiUkxYEJcfcTIYSUBBQwWSBcjoJ3IcXzKfpzYPS7kIzsQgKUEK86AyOLIH+yCLpqVgvmn9IkZ2B6h8Oy4LGakI6AmTC6HABwpD+YNKcmFQd7lRAvHRhCCCktKGCywFckk3j1tlHLk3hDEU1bsxhkl0uIV5ShKnU6mar9Hvzma2fiL23nye6NXW5GKBx7Xd74viYAqPJ7UF/pBQDs6cxcvopGJbTH26jH1JbB5YxdMzMwhBBSGlDAZIFfNciuULNPJElSMjAqASPKSZKkHUInBtnl0kadqoSUiFgEadcaArmE5Nb+8RUt1LuOZC4jHRkIIBSR4HDEgsBuFx0YQggpJShgssCvmnybOGo/XwTCUQjtpL4e9Uh/dSeS6WWOJktIel9rmwOjU0ICgIlxAWMkyCvyL41VPnhcTrmEFI5QwBBCSClAAZMFasejUGUk9fdVX4/L6ZAFhHqdgNlBdqIMFNBto04vgoSwsEvcpRYwsRzMriOZS0hiCu+YmjIAUGVgGOIlhJBSgAImCzwup9y1UqhOJPF93U5H0o28zKMN8kajkixmjLdRi1k3egLGk/ZrFQfGHjdDETAOzeMTTXQiqVuoATADQwghJQYFTJbI+5AK5MDobaIWyEHeeAlpKBSRy03G26hj59AMshs2VoaSW7DtysDEQ7yJwk20UhsqIcUdmObquAPjYgmJEEJKCQqYLBFh2UK1UsuLHHUm65YlCBhRPnI6tHmZdOh3IRmbJZO/DIzWgTHTSi3vQZIdGE7iJYSQUoICJkt8BR5mNyRvok5+C4UDs+VALz7Y14339nYBiOVfHA5H0vF6KF1ImVcJJOKx2YFJlYFRt1LvzpCDYQaGEEJKGy5zzBIhHAod4tUvIcXe1nuf26J93GAHEqB2YJTX12+wjVr+WpscGFHm8bqSxduE0RU40h/E7s4BTB9Xk/IcYgZMc03MgXHTgSGEkJKCAiZLROfPUIEFjF9HwFx19nh09A5r8hwOB3D1rBbD50/XRl2VoY06b3NgdATMxNEVWL+nC7vTzIKJRCV5E7VcQuI2akIIKSlMCZhly5bh6aefxkcffYSysjKce+65+NnPfobJkyfLx/z2t7/F448/jvfeew99fX3o6upCbW2t5jwTJ07Enj17ks79r//6r/LnH374Idra2vDOO++goaEBt912G773ve9l8RLtwS8vOyxQiDcY1VyHms+fPg6fP31cTuf3Jbgo0ahkeCGkN15esz0D49YTMLEczLpdnThhY6Xu1/cNhxGJSnA5HWisogNDCCGliCkBs3r1arS1teGss85COBzG97//fcybNw9btmxBRUWsA2RwcBALFizAggULsHTp0pTnuvfee3HjjTfKn1dVVcn/39vbi3nz5mHu3Ll4+OGHsXHjRlx//fWora3FTTfdZPY12oJSQipUG3VqB8YK5AxM/PUNqHYrZQzx2uzApArxAkBrQ+zP4RvbO/HG9s6052mu9svhXRd3IRFCSElhSsC88MILms+XL1+OxsZGrF+/HnPmzAEALFq0CACwatWqtOeqqqpCc3Oz7nOPPfYYgsEgHnnkEXi9XpxyyinYsGED7r///uIRMAXeh6S0UduTw07MsYhVBG6nQxY3mb7WPgcmdQbmoimN+MypY9ARLxGlwgEHvnL2ePlzN7dRE0JISZFTBqanpwcAUFdXZ/prf/rTn+LHP/4xWlpacPXVV2Px4sVwu2OXs3btWsyZMwder1c+fv78+fjZz36Grq4ujBo1KpfLtgRlH1LxhXitQHRZCRelPxBrS67wZe5ksnsSr7gmvQxMudeNX199hulz0oEhhJDSImsBE41GsWjRIpx33nmYNm2aqa/91re+hTPOOAN1dXV48803sXTpUhw8eBD3338/AKC9vR2tra2ar2lqapKf0xMwgUAAgUBA/ry3t9fsSzKFLGAKtAvJ7hKSN6GNuj/uwBgZhKdso7Z7Eq917pObk3gJIaSkyFrAtLW1YdOmTVizZo3pr7399tvl/z/11FPh9Xrxz//8z1i2bBl8Pl9W17Ns2TLcc889WX1tNuSzjToalXDVf6/DW7uO6lyHTQImYZaLaKE2ImA8Oh1MVpIuA5Mt3EZNCCGlRVa/wt5666147rnnsHLlSowbl1u3CwDMmjUL4XAYu3fvBgA0Nzejo6NDc4z4PFVuZunSpejp6ZE/9u3bl/N1pSOfbdSdA0Fd8eJyOnDWRPPlOyPIu5BkB8b4NmvFgbE3A2OtA8MuJEIIKSVMOTCSJOG2227DM888g1WrViWVebJlw4YNcDqdaGxsBADMnj0bP/jBDxAKheDxxBYHrlixApMnT06Zf/H5fFm7N9ng11l2aBfDqqm7a5ZcJD/ucztR5U+/WDFb5G3U8UF28hReA9/P7kF26ebAZIuLk3gJIaSkMCVg2tra8Pjjj+PZZ59FVVUV2tvbAQA1NTUoK4uNZG9vb0d7ezu2b98OANi4cSOqqqrQ0tKCuro6rF27Fm+99RYuvPBCVFVVYe3atVi8eDG++tWvyuLk6quvxj333IMbbrgBS5YswaZNm/DLX/4SDzzwgJWvPSfy2YUkXJ4Krxv1lfkRaYmD7AbkNQLGHRjbSkgixOu2sIQkMjBc5kgIISWBKQHz0EMPAQAuuOACzeOPPvoorrvuOgDAww8/rMmiiPZqcYzP58MTTzyBu+++G4FAAK2trVi8eLEmF1NTU4OXXnoJbW1tmDlzJurr63HXXXcVTQs1kN8uJLGU0a68ix5qF0WSJKWE5DWQgXHZ68CI0pReG3W2cJkjIYSUFqZLSJm4++67cffdd6d8/owzzsC6desynufUU0/F66+/buby8ko+B9kNxQVMuc7mabsQbdSSFMucKCUkA11IYg6MXQ5M1IYMDEO8hBBSUnAbdZYobdT2OzDyzJe8Chjlj0YwElWVkIw4MA756+wglGYOTLYoDgwzMIQQUgpQwGTJMV9CUomDYDhqqo3aZ/skXuvbqD3xDEyEGRhCCCkJKGCyRIiJ9p5h/PWDA/i4o8+27zVk89RdPZxOh2artNJGbSIDY9scmPgqgQwrDczADAwhhJQWFDBZIuah7O4cxLf+9D4u+/Ub8k3eaoSAyWcGBtC2UotljoYm8coBYHvEgB1t1MzAEEJIaUEBkyVnTqjDl84ch3MnjYbX5cRQKIID3UO2fK+huHjIpwMDaFupTU3ilR0Ye8prdqwScHGZIyGElBQUMFnidTtx3xdm4PEbz0HL6HIAwOG+QIavyo6hYOym6s+zAyM6kQJZlpDs34Vk5RwYOjCEEFJKUMBYQEN8uNyRfpsEjCghFciBUQuYKgNt1D67dyGFrW+jZgaGEEJKCwoYC2ioigkY+xyYeAkp3xkYlRAZiG+jNufA2N2FZP02ajowhBBSGlDAWIAY72+bgAnlv40aUJyUYVWI19AyR5sdmKCN26jDUcnQwEZCCCGFxdQkXqKP7MDYVkKK3bDz3oUUFyI9gyGIe3qVL/MyR9sH2dniwChiKCoBVmij3uEQugdCAIDmGr+lbd+EEDLSoYCxgLyVkPKdgYkLhM6BIADA6VBWKKT9OrszMDbOgQFi03hdztx+1jsP92PBL1+XfwbHN1Tg5cXnw+m0zjUihJCRDH8ltID6Si8A+0tI+c7A+OKCqSsuYCp8bjgcmW/A3hLOwADWbKTecrAXwXAUQq/sPDyAPpvmBBFCyEiEAsYChANzpD9oy/mHCrBKAEh2YKoMBHiBfDgw1mdgtA5M7gJmMB56Pv+kBgjNF8jD3ixCCBkpUMBYgBAwRwcCtnSxDBZgGzWghHiPDsScJSMdSEA+5sDES0g2ZWCseA8H42W/cp9b/jkG8rC5nBBCRgoUMBZQV+6FwxELf3YOWF9GGi7ALiRAETBrth0BAFQamAEDqFcJRC3v6IlEJVlgWFlCcjodcrnHio3UA3HRWeF1aQYCEkIIsQYKGAtwu5wYXRHLwRzps76MVKg26gmjKwAoN+Pj6ysNfZ1aWFjtwqhzNR6Lu3qsnAUzJLtmKgeGJSRCCLEMdiFZRH2lD0f6g7a0Ug8VqITUduEkzJ40GsOhCNxOB86YMMrQ1/lUwiIYiVraLaQRMBZmYIB4DiZiTYh3QDV80OdRJhoTQgixBgoYi2io8uGj9j5bOpEK1YXkdjlxdmud6a/TODDhKOCz7prUjo7HabUDY90+pCG9EhIzMIQQYhksIVmEXfuQQpGofNPOdwYmW1xOh9zVY/UwO+HAuJwOy2equORpvNZlYMpYQiKEEFugA2MRdg2zEwFeIP8OTC54XA5EopLlrdTifFaXjwDFgbGijVoMH4w5MPaXkKJRKen8LqeD038JIccsFDAWYdc+JFGKcDqsbRu2G6/LieFQ1DYHxsoOJIG8kdqKDExAKfvZ3YUUDEfxmV+9jo87+jWP+z1O/PfXz8SnTmyw5fsSQkghKZ07YpGjDLOzWMCoWqiNTMEtFsRv/lZP47VjBozAyi6kwZDIwLiVEG/InhLS/q7BJPECAMOhKN7c0WnL9ySEkEJDB8Yi7CohFSrAmytCYFhdQrLTgVFvpM6VwfjagPI8lJB6h2Pfa2yNHy9/53wAwC9f2YbfrN4pXwchhBxrUMBYhFxCstiBGQyWpoDx2ObAxAWM23o3ymVhF5I8Pdnntr2E1Dcc23hdXeZBuTf2V7q2LDaXqD/A4DAh5NiEAsYihAPTPRhCMGzd7JPhYGGm8OaKcGCsvmmLEpItDoycgcn9muVVAhoHxh4x0TsU+17Vfo/8WIXPpbkOoyx/YxdWbj0MABhd4cXdl52iOS8hhBQLFDAWUVvmgdvpQDgqoXMggDE1ZZrnD/cF8I+DvbFjyz2YflyNoUzLUIHWCOSKXfuQhANjRwbGFc/AWFJCUg0fFBOU7ZoDozgwyl/nirgT02+ihDQUjODe57ZA/fJnThyFa2ZNsOZCCSHEQihgLMLpdKC+0of23mEc7lMETCQq4dE3duEXL30sixEAePirM7FgWnPG85ZqCcmujdRBOzMwFpWQIqqWZu0qAbsyMDEBU6XrwBh3fXYe6UdUAqr8bpxz/Gis2NKBre191l4sIYRYBAWMhdRXedHeO4xb/viefAPpGw7jYM8wAKClrhyDwTCO9Afx/r4uQwKmVB0Yr8umDIyNc2BcFs2BUZdtyjWrBOwpIfUNixKSyoGJbw4fMOHA7Dg8AACY3FSFS6c3Y8WWDnxEAUMIKVLYRm0hp4ypAQB80j2Ejzv68XFHPw72DKPK78ayK6Zj1XcvQNuFJwAA9hwZNHTO4VLtQrLJgbEzAyNEUSTHSbyDqtk9PrfT9hBv71CyA1OeRQlp+6FYK/akhkqc1FQFANja3mf5RnFCCLECOjAW8uPLp+ELZ45Lch2mjqlGbXmsK2RifWzD8+7OAUPnlEtIntJ6q4QYsGuQnR0TZq1zYJQZMA6HQykh2ZaBiTswqgxMZdyBMVNC2nE4LmAaK3BCYyVcTgd6hkLo6A2gucZv4RUTQkjulNZdscjxup04a2L65YcTRysCJhqVMu7zGZIzMKVlltmdgXFbvAcpdk5rBtmJso1wzWzvQkqTgTHjwOyIOzAnNFbC53ahtb4C2w/1Y2tHHwUMIaToKK274jHAuFFlcDkdGA5FccjA0LvhEs3AeOzKwORhlUCunVMityRyKD6PzSWkYZ026ngJKRiOGnoPIlEJu47EXMFJDZUAYlkYANja3mvp9RJCiBVQwOQZj8uJ8aNiHUrihpGOwRKfA2N5BkaEeG0oISldSLlds+zAeLQOzLBNqwSUDExyiBcABg0MszvQPYRAOAqvy4lxo8oBAJObYwKGQV5CSDFCAVMAJsTLSHsM5GCUVQKlVe0rxV1IVmVgRNlPlHHsn8QrMjCKA+N1O+Uc0oCBYXbb4/mX1voK+ecgBMzHHRQwhJDigwKmALTGg7y7zAgYT2m9VR67HJiofW3Ubpc1c2AGglrRmb85MFqRa6aVWp1/EYgS0raOfkvWKxBCiJWU1l3xGGHC6JhFb6SVeqjUB9mlyJMEwhF81N6Lj9p78Un3kOHzhsJ2rhKIT+LNNQMTdzwqRIjXxm3U0agkB3WTBExcQA0Y6ESSO5AaKuTHWurKUeZxIRCOGu6aI4SQfFFadYljBDOt1EPB0iwhZQrxfvHhtfhwf4/8+WWnjcUPPzNVXoqZClu3UVs0iXcgQXSKEpLVbhQA9AfDEGNaEncWiRKWMQcmHuBVOTBOpwMnNVXig/092NreJ4d7CSGkGCitu+IxgrqVWpKktDuRSnYSb5o26nAkKouX0RVeHB0M4tkNB7D648M4ZWy15thJDZW4+7OnyO3mpTYHBrC3hCTyL16XU965JBAlJCOt1IoDoxUpJzVV4YP9Pfj5i1vx2Ft75MenNFfjzoUnG9rnRQghdkABUwDUrdSZhoSVahu116UVHGp64l0zAPDW9y/G5gO9+NenN+IfB3vxxvZOzbFvbO/EF2eOx/RxsSnHyi4k+zIwuW6jHgwom6gB2LpKQHQgqYfYCYSAStxI3R8I493dRxGNWzfDoSg6B4IAgONVJSQAOHPiKDy1fj92HRnQdM29sb0T18xqwfF0ZQghBYICpgB4XE6MG1WGPZ2D2N05kFbAHIvLHLsG4zddvxtulxMzxtfir7eeh9e3HZYdBQC474Wt+KR7CF2DQfmxfMyBydmBCYlN1MKBsW8btfh5VSWUjwD1MDutcLr9yQ14aUtH0vHH1ZbJ1yy48oxxaKz2y0IJAO78yyb0DYcxbNNkYUIIMQIFTIGYOLoiJmCODOCc40enPK5US0hyF5KOm9EdFySjKrya4y+a0qQ57rG39uKT7iGNY5OPEG+uGZgkB8bWEpIiBhORHZiEEpJoiz6hsVIOGjscDlwzqyXpHG6XExdObtQ89tP/+wh9w2GEc5yXQwghuUABUyAmji7HagAvbG5HSHXDbKkrx/knNcifDx/DDozYD5WKmvhcE42AsbOEZHEGJnGVQDASNbQ+wgx6awQEqdqoRbno4a/O1LRNG8XtsmZiMSGE5AIFTIEQ2YFVWw9j1dbDmueeu+2fMO24GkiSJJcjStWB0cvAiJLQqPLkm64aIWDETRpQZ2BsKCFZvI1aHmSneu+CkSj8TuveS71FjgJZwKjaqIPhqPw1oyvSC8hUeOR2czowhJDCQQFTIC4/7Th81N6HrgEl37H5YA/2HR3Cqq2HMO24GoQiklzOKDUHRu06JCJe86icHBj72qhzd2DEKgFtFxIQy8EkdgvlgrxGwKfjwHiT26iFeHQ5HfLP1yxWZYUIISQXKGAKRE25B8uumK557H/W7sZdz27GG9s7cetFJ8r5F6CEHZhw8k1OKSEZdGA0AsbOVQIWZWASHBi30wGnA4hKohMpO+Ggh1kHprNfEY/ZlrLc8Z89BQwhpJBwEm8Rce6kegDA+r1dGA5F5CF2LqfDlsyHnQiBEUgX4s3gwIhgqq4D47YvA5NrtkMImHJVQNaufUjpMzDJDkznQGwDerblI0DJH7GERAgpJBQwRcSkhgo0VfsQDEfx7u4u2YEp97hKbmCY2BYd0g3xGszAlAsHRrkB56ONOvcMjOhCUlwRu2bB9AoHRq8LSWeQ3dF4+a4uBwFjldAjhJBcoIApIhwOB847IebCvLHjiOzA+Ess/wIoDoxuBiZeQhqV4Saqn4Gxs43a2i6kctX75o87MFbPTpEzMHoOjM4gO1FCqqvMQcDIJSQ6MISQwkEBU2ScFy8jvbn9CIZCIgxaggLGnXoSr9ESUroQrx0ZGHFjziUDE41KKgGj58BYe9NXMjDp2qgV10c4MNaUkOjAEEIKBwVMkSEcmI2f9KCjN5ZXKC9JByb1AsOjA8ZCvGI5oVrAiPMVqwMzrCoRqd83ZZid1SUk4cDolZD0MjBCwKRfmpkOMfAv1aJOQgjJBxQwRUZzjR/HN1QgKgH/9twWALC07TZfeFI4MJIkmXZg+oZDiMZFhZ2D7OQMTA7OgtrtUDtndoV4ZQcmTQlpQJOBiYniXEpIsgPDLiRCSAGhgClCPn1ybKT+gZ5hAMD4uvJCXk5WyF1ICTfs/kBYvvFl7EKKC5ioBPTHcxxyBsaGbdSKA5O9yBC5pTKPS9OmLDswtmVgUod4B0MRWQCKDEwuJSQ3B9kRQooAzoEpQhZ/+iSc3VqHQDgKl9OBcyel3pVUrKSaxNsdD/D63M6Mw/n8Hhd8bicC4Sh6BkOo9ntszcBYMaBtIC60RPlGYEcXUiAckQWirgMTvwZJiu3UqvC5relC4ioBQkgRQAFThPg9Llx8clPmA4sYX4pdSF0Gy0eC6jIPDvcF0DMUwngogshtxy4keZVA9jfmVNvD7SghqTd3V+o4MGUeFxyOmIAZCIZR4XOrMjC5t1Hnswvp444+LHpig2atBABMHVONh746UxafhJCRAwUMsQXhwESlmCAQNxijLdSCmriAEaUSO0O8Lrk0kouAiTswXu1fLaWEZJ0DIwRMpc+tewN3OByo8LrRHwhjIBBBqDwqB6Jzc2DyP4l3xZYObDnYm/T4/q4h7O4cwKQG80spCSGlDTMwxBa8qoyK2oVR9iAZG6efuNDRzlUCHqedDoz1bdRC1OkNsROoO5GE++V0ZN4Eno5CtFGLP0MLTmnGX9rOw1/azpNfN7uhCBmZ0IEhtqB2SIKRKMoQu5GaLSElzoLJxyTeXEojqR0Y+0pIekPsBLEgbwADgTCODsRe36hyb04ll0KEeMV70lzjx2njawHESq29w2HOoyFkhEIBQ2xB3eascWAMLnIUqPchSZIkly3saKO2NQPjsa6EtL9rEH96ey8+7ugHoL/IUSC3UgeV7q9cykeAKsSbxxJSOJL8vqcKihNCRgYUMMQWHA4HvC4ngpGo5gZjdAaMQO3AqLte7GijdskD2nIQMPE5MBU2lpB+9cp2PPnuPvnzxip/ymOVElIE/fFry1XACOGQTwdGvCdulfPGeTSEjGwoYIhteN0xAZOLAyNnYIbCGiFkyyoBSzMw9pWQDvfHhtFdOLkBp4ytwRdmjkt5rHqY3XDc/RmdwxA7oDDLHEUJyaMqfbnpwBAyoqGAIbbhcSVP4zXrwFRrHBjlPMWSgfnv13Ziw75u+fOP2mOdMqkdmNxLSGK79BdmjsfCU8ekPVbehxSMoGcw9xkwQGGWOeo5MHI7NzMwhIxITN0Fli1bhrPOOgtVVVVobGzE5Zdfjq1bt2qO+e1vf4sLLrgA1dXVcDgc6O7uTjrP0aNHcc0116C6uhq1tbW44YYb0N/frznmww8/xKc+9Sn4/X6MHz8e9913n/lXRwqKR2carxikNqrCnAPTMxSSN1s7HbBl7odZB+ZQ3zD+/e//wPMbD8ofOw4PAIiFTdUoGZjcb/r9on06TfeRQFnoGLZkDxKguCD5FA5hnfk/Hm7FJmREY8qBWb16Ndra2nDWWWchHA7j+9//PubNm4ctW7agoqICADA4OIgFCxZgwYIFWLp0qe55rrnmGhw8eBArVqxAKBTCN77xDdx00014/PHHAQC9vb2YN28e5s6di4cffhgbN27E9ddfj9raWtx00005vmSSL0QrtdaBic+BycqBEUFOe7r/zc436Ym/lnKvC0sWTJEfr/S5cel0rTNiZQlJTPut9GXekSWcoIFAWFkjkGsJyZV7VsgscnjbqXJgOBGYkBGNKQHzwgsvaD5fvnw5GhsbsX79esyZMwcAsGjRIgDAqlWrdM/xj3/8Ay+88ALeeecdnHnmmQCAX/3qV7j00kvxH//xHxg7diwee+wxBINBPPLII/B6vTjllFOwYcMG3H///RQwJYTIqWgzMNmFeHuHQwiF7VsjAJh3YAbieZdR5V5ce+7EtMdaWkIaFusKTDgwwbAlawQAdXg2nyUkHQfGgsGDhJDSJac7QU9PDwCgrq7O8NesXbsWtbW1sngBgLlz58LpdOKtt96Sj5kzZw68XuUf2vnz52Pr1q3o6urK5ZJJHlEcmNgNJhCOyCFX0wJGlYGxowMJML8LSWx5Ttx7pIeyC8mCElJAmcCbCXUXUqfYRJ2jgHEVpISkk4EpgJAihBQPWYd4o9EoFi1ahPPOOw/Tpk0z/HXt7e1obGzUXoTbjbq6OrS3t8vHtLa2ao5pamqSnxs1alTSeQOBAAKBgPx5b2/y2HGSX0SpJxiJiRZRPnI69Lcn66HOwIibv9umvTdKKNTYDVERMJlfi1xCyjEDE4pE5Z+DMQGjZGCOWpSBKUSIN30XEh0YQkYiWQuYtrY2bNq0CWvWrLHyerJm2bJluOeeewp9GUSFcGAWP/kB/B6nXJqpLffCaVCEiAxMKCLJ6wTsysCYdmBSTN3Vw6oSkhBNgMESUvzaVm09LIegc83AFCLEqzsHxqTgJIQcW2R1J7j11lvx3HPPYeXKlRg3LvUMCj2am5tx6NAhzWPhcBhHjx5Fc3OzfExHR4fmGPG5OCaRpUuXoqenR/7Yt2+f7nEkf0xurgIQc086egM4Eg+RTj+uxvA5KrwuWViIr/faVEISI/INZ2DE0DojJSSLQryifORzOw0JuSljquB0QBYv40aVGS7fpUJ2PvI5iVc4MC61A2P/ROD393bht6/twG9f24G/vP9JTjOCCCHWYsqBkSQJt912G5555hmsWrUqqcxjhNmzZ6O7uxvr16/HzJkzAQCvvvoqotEoZs2aJR/zgx/8AKFQCB5P7DfwFStWYPLkybrlIwDw+Xzw+XKzxom1/Ntl03Dt7ImaUoMDDpzYZHxzsMPhQE2ZB0cHguiMD3CzY40AoM5UmM3AGHBgLMrAmMm/AMCU5mqsW3qxPPyutb4i5xZ0ZZljAebAaLqQ7J0IHI1K+Pojb8s7p4BYfmjOSQ22fD9CiDlMCZi2tjY8/vjjePbZZ1FVVSVnVmpqalBWVgYgllFpb2/H9u3bAQAbN25EVVUVWlpaUFdXh5NPPhkLFizAjTfeiIcffhihUAi33norvvKVr2Ds2LEAgKuvvhr33HMPbrjhBixZsgSbNm3CL3/5SzzwwANWvnZiM06nQ3ZhckERMDEHxrY2arNdSAHjJSS/nIGxpoRkZAaMoLHaj8bq1OsGzOIuQPeP7hwYm0tZwUhUFi/1lV4c6Q/iUF8gw1cRQvKFqTvBQw89hJ6eHlxwwQUYM2aM/PHkk0/Kxzz88MM4/fTTceONNwIA5syZg9NPPx1//etf5WMee+wxTJkyBRdffDEuvfRS/NM//RN++9vfys/X1NTgpZdewq5duzBz5kx85zvfwV133cUW6hGKWOh4RHZg7M3ARKISJCnzTVG0UefTgRE3VCOiyS6U0k0+Q7zJyxyVUpY916F24qbFy57DFizjJIRYg+kSUibuvvtu3H333WmPqaurk4fWpeLUU0/F66+/bubyyDGKCPJ+0j0EwM45MMp5I1FJ89u+HooDYyQDY42AEbkbMw6M1SglpAKEeFXvkd3XoS5NiZIdBQwhxYM9dwJCLES0Ur++7QgAZBQW2eJSnddIDsaUAyOHeC0qIRnMwNiBW97anUcHRqeEpJSy7HdgKGAIKT64zJEUPQunj8Ga7UcQCEXhdjkyLjDMFvV8GUMCxswgO9VQv0hUyjpI22ciOGwXZsPOqXh922E8/tZeTebI6XDgK2ePxwWTtbOilBKSzioBmzqD5OF5Tgf8nth7PGzBLitCiDVQwJCi55LpY3DJdHtEixq1qIgYKEtk04UExFYrlBkoO6X7nsXgwOTqfPzHSx/jA9Umb8H+7sEkASOvElC9Rx6bu5DU6wuEgBmiA0NI0UABQ0gcrQOT+aZoZpCdOrcTCEeyFjBKG3V2X28FVjkwQozd+KlWtNZXYnfnAH772k553YSasM4iT/F+2TWJN6JaIOmPC1CWkAgpHihgCInjcDjgcjoQiUqGWqmVQXaZ/xq5XU64nQ6Eo5JuGeI/XtyKVR/HBjyOrvDhF1+agfrK5LlGioDxZPyedmHVEkWx5POS6WNwRssovLe3C799badutkYISrdOF5JdKw3EeV0ulpAIKUYoYAhRIQSM1RkYIJaDCQcjSUHe7Yf68OuV2zWPvbS5A1fPasn5e9qBVUsUxc9BuFPiv6Fw8s9etwspzRyYTZ/0YNuhPs1j1X4P5pzUYLgNX/09y2QBQweGkGKBAoYQFW6nA0EYcxfMDLIDAJ/HhYFgJKmV+om3Y2svzp00Gj63Eyu3HsbhFAPT+uNzYIwuw7QD0b6ca+lGODCiPONxpe5uEjkX3TkwCdfRNRDE5//rDd3ru+/KU/Gls8Ybur6IavYMS0iEFB9soyZEhbLQMb27EI1KGAwZLyEBqlkwqjJEIBzB0+9/AgC44Z9a5T1RYmhfIv3F0IVkUYhXCDmvK+ZuCHES1Dmv6DTSLHNM4QR1DgQQikjwuBz41In1+NSJ9RhTE5tEfKBnyPD1CSHlUnch5dgGTwixDgoYQlQYXScwFIpAzHU0U0ICtLNgVmzpwNGBIJqr/Tj/pAY0VMVyLykdmGIQMBa1LwsBIzq0vG4DDoxTPQdGv4QUjJegRpV78b83zML/3jAL809p1j02HerWbbkLSSdgTAgpDCwhEaLCJdyFDDdnUT5yOCDnIzIhhtn1BcJyKUKUj7545ji4XU45uHs4hQMjvm9VAQWMFe3L4UhUFolC2HlTlIQiUQni7XBr5sDoCx7h4Ki3lqcTR6mvkXNgCClmKGAIUSHKEpkcGHkKr9cNh8PYUDrhNHzj0Xc0jzscwJfOjOUyhAOTuoRkrmxlB8L5iEqxUpozi6F86jKREBdCGIkuMJfcJq0cq1nmmKKdWxzv1Sk36ZWnUiF3ITkd8MevkSUkQooHlpAIUaFkYIw5MGa6geac2KD7+OdPOw7j68oBwEAJKQSgwIPsVMIg20WKQVWQWQgNj8oxUYsW9XvhUXUhpVppIM7t0QiY7B0Yj8spz+0ZZgmJkKKBDgwhKpQMTPobndkOJAD47vzJ+JcLJ0GtjRzQuimihDQYjGAgENY8F45E5RJGIQWMuhMoHJGQzaWI/IvL6ZAFkfq8wUhULtuEUzgw7hTLHPVKSJ40LdqpCMvBYXWIlyUkQooFChhCVLgMTneVp/CavHuXZxA8FT43yr0uDAYjONIf0JxfDM7L5vtaiXrlQrbD7IRL4lOLDJW7ElIJBfV7obtKIEFshmQHRjnWm5UDo6wv8Ls5B4aQYoMlJEJUiLJExgyMnEWxfqCcHORNKCP1x0WT1+3UuAv5Ri00sh1mJw+xU70Op9Ohux5AnsLrdGjyRqlWCeg7MOYzMHLrttMJvzd2rlj3mT2rCwgh5qCAIUSF6QyMiRKSUVLlYMQQu0KWj4CY0BBGSLb7kEQpzJcgxPSyKnI3kMuhe2ySAxNJzsCk6lhKR0S1vkCUkCTJnAgihNgHBQwhKtwuYxkYO+exNFTqdyL1F8EmakE2gkCNEAGitVyg55TIgsSp/ecqZQZGHpCnaqNO0aKdjpC6jVp1ncNBChhCigEKGEJUpBqOlojYmGxLCanKCyDZgRkogiF2gnR7iIwgphEnlsL05rWow7RqUnYhxa9JU0JyJ7dkZ0JxfpzwuByyO8dWakKKg8L/S0hIEWE8A2NjCakyNvY+cZid4sAUbpGjIObARLLOwCgOTIKA0ekWEqLD7UosN6WYA5OmjTpoootIuHAeVyx743c7MRCMMMirIhCO4NV/HEJf/M+m4KSmKpw2vrYwF0VGDBQwhKgwnIGJB2rL7SghpcrAFFEJKdeFjoFQcogXUGbBBHUyMN4kAePUPC8I6mRglLyM+RKSmM7sjy/jHKKAkXni7X340V83Jz3ucTmwZslFaKr2F+CqyEiBJSRCVLhTLAhMRHQh2eGG1FfGS0j9Qc3jIsRbDCUkZaFjlgJGp40aSBHiVYVpNdeQorNIODCaVQLZtFELByYuarlOIBmxHHN8XRkumtKIi6Y0or7Si1BEwpptRwp8deRYhwKGEBUug9kOUULKNNclG+R1AikyMFX+IhAw8kLH3CbxJod4k4WGOkyrd2ziTia5jdqVPDPGTAkpMXvjj6+CYAlJQWSZPjdjLB657iw8ct1Z+MLM2FqMN3ZQwBB7oYAhRIXRbdSihGRHOUddQlLPHBFzYOzI3ZglVfnGKAEdlwRQRIdeG7UnoYSUcht1mjkw2YR4RQlJrBNgCUlBzPNRd2n90wn1AIA3t3dyZg6xFQoYQlS4DW6jtnOpohhkF4xE0TushCPlOTBF4MAoTlW2Dkzs55eqhKR2SkIpSkiyW5M0iTdZ8Hjk7iYTqwQiSogXUG7SAQoYGXmej0f5WZ85cRS8bifae4ex88hAoS6NjAAoYAhR4TK4jXpQ7kKyPgPj97jkMpE6yDtQRCFet8GwcypSOTCygFFP4pVLSAbnwEQimnMB2WZgtN+XGZhkZAfGo/w98HtcmNkyCgDw5naWkYh9UMAQosLojdnumSwNOusE7ByeZ5ZUU3CNkjIDI5ySsLqElLzbCNC6ZepShXBgdJc5ZiNgXNoQL0tICqkmKp93wmgAwBvbO/N+TWTkQAFDiAqX0W3UNg6yA4D6quRpvMXURu3OtY06RReSXgYmFNV3YDRbsVWCMySHeHV2IZkI8crzZ5wM8aZC/CzUDgwAnBvPwazd2ZnRzSQkWwr/LyEhRUSqBYFqJEmy34HRmQWjtG4X/q+tJ+c26vQZGG2IN1UbtWqpZESCuIcGdBwbTxarBCKyA6MtIdGBUUglRE89rgZVPjd6hkK46rfrUF3mxm0XnYgZHG5HLKTw/xISUkS4DEziDYSj8m/8drRRA0oJ6b29XWjdWgFAETPFEOI1Oi8nFcEMc2D0MjCpupCAWJC3DDGBocyBUVyBbEpIie3bZczAJCEcGF+CA+N2OTFncgOe//Ag3t59FABQ7ffg/i+flu9LJMcwhf+XkJAiwkgGRuxBAuwJ8QKKA/Pchwfx3IcHNc8VxxwY846GmpRt1Dq7kOQupBRzYACtExTUdWCU9zUaleBMOJceic6PKCGxC0lBvI9+d/Lfg59cPh2XTGvGyo8O4/+9t18+lhCrKPy/hIQUEUa2UYvykd/jTNrPYxWfmzEWa3d0omcopHn8xKZKnNRYZcv3NIMn5zbqDIPsNCFefQfG5XTA4QAkSXsdIb05MKr/D0Wj8DkzC0/hwnmcLCGlQnFgkv8e1JR78JlTx6J7MIT/997+rDeXE5IKChhCVBhxYAbyMFBufF05/vjNWbadP1eUSbz2D7ILpcjAADFxEYxENdchdyHptFEDMUFkJEYkzulKWiVAASNI58AIPAZHExBiFnYhEaJCzsCkKY3YHeAtBdwpxvgbJVOIV5OBSdGFFLuOZCdICfEmt1EDxnMwSYPsmIFJIp0DIxDvW7Zil5BUUMAQosKQAxPvBiq3Kf9SCngM7oxKhdy9knDjk7dRG5gDA+h3jektc3Q5HRCxl8Tlj6kIJ3Uhxf7LEpKC7MB4Uv9d0BOZhFjByP0VkhAd5BH5OhmYQ73DeGr9fmw+0AOgONqZC4VL/q06WwdGzGoxscxRr4SkM1AvqOPAiM8D4ajh4HE4YQ5MGUtIGqJRKWU3mRqj6zkIMcvI/ReYEB1SLXPsHQ7hK79dp9ntUlfhzeu1FRNyriFLBybVjU93maPchZSuhKQzyM6tFTxeIWAMdsOkmsQbYAkJADRdRekcmFz3ZhGSCgoYQlS4dG6I0aiE7/z5A+w8MoDmaj8WTGuGx+XAl88aX6jLLDh2hXiVDIxeF5JeCUnHsUnl7ridQMB4BkaZxMsSkh4ixwQA/jQODEO8xC4oYAhRIVpm1+3qxO1PbgAAHBkI4rWPD8PrcuI3X5vJaaJQlQVs2katybREtFkU7fHJmSW5hJTgwMjrBAxes9xGnRTipYABlDCzy+lIO07AZWC6NSHZQAFDiApRFtp3dAj7jn6iee6ey06heImjJxzMoIR4DSxzjJeQPDrD59w6mZmg7MAkTu41N3xPHOdK3EYdpoABVJuo07gvgCJK6cAQq6GAIUTFZ2aMQVSS0D2oHSA3ZUwVPnViQ4GuqvjQEw5mCKQQGb60IV6dDIxON1SqEK/elN90yNkb4cDEZ50MBZnlAFSbqNPkXwBVp1iWgW9CUkEBQ4gKn9uFL545crMtRsm1jTqYso06ucyTapkjoN+FJARPcr4mfiM1GuIV2Zu4A1PmFSFeOjCAcQdGL2hNiBVQwBBCTOPWEQ5mEDe/RAdGdxt1wkh/7XVo8xWRqCSXKlKd2+wcGGUSr3UhXkmSsGFfN470BwEA04+rQXONP+fz5hPjDgxLSMQeKGAIIaZJFA5mkCQptQOjG+JNv0oAUH67Vwsfj4GAcDqSJvHGS0jhqIRwJJrTHqy1Oztx9X+/JX/eXO3H2qUXweHIvGSyWEg1TTkRJcTLEhKxFk7iJYSYxpNDF1I4KkH8Mu5LaHUWrol6Eq8iYNLMgYk7QWp3JbHtWpzb6DUnTuItU01eHs5xs/K2jn4AQHV8s3h773DJDXoTDky6GTAAQ7zEPihgCCGmkX+rzuKmpB6AltqB0ZkDk7YLKe7AqM6dVELSydekQ3xfEUJVOw1DwdzKSJ0DsdLRp6c2y48FchRF+YYODCk0FDCEENN4cthvE0wnMnRmtYSiaebAJEx5DarKPonlGNMlpIQuJIfDId+sc50Fc3QgAAAYW6vkXoIlJmCMOzAcZEfsgQKGEGIadw5lAfGbu8flgDPBVfHotDqnXeaYMBE4FNYP8Ma+3mwbdfIWbLkTKcdZMEfjDkx9pU9+XbmeM9/Im6gzdiFxGzWxBwoYQohp9LZAG0XZg5T8m7sQHkKIAOpSjl4GRptrCUbi4kjnpuo1K2ASSkiAdbNgRPdRXYVX/jmU2o4lI5uoAfWsntJ6faT4oYAhhJhGb/6KUVLtQVKfVzPILqGUozk+YR5NMCzG/6deO2C0VKPX/SRaqXOdxiscmNEVXtnBMJrNKRYMOzDx9ygqxfaKEWIVFDCEENPk0kadahM1oJ+BSbvMUS5PROPXoz/hN/b15jIwyi4k5VxW7UMSAqauUhEwx64Do/z8Sq3TihQ3FDCEENPksswxXfeK3rj/xK3QajwJU16F8NFzd8ysP5AkKWmQHaDcrHPpQopEJXQNKiUkca2lloEJGM7AKD8/BnmJlVDAEEJMk8syR+E06IkMr45Losxj0XFgEoRUKMWOpdhjxtt51a9LPQFYKSFl75Z0DwYhxU9fV67KwJRYF5JRB0YtALkPiVgJBQwhxDSJ81fMEIikDvGqh56J39aVLqQ0qwTix4pzi5kveuc2kjVROwVq4VQmSkg5ODBiBkxtuQdul1OehVN6bdTxXUie9LcR9fsW4T4kYiEUMIQQ0yTOXzFDOgdG3T0knJKQTjeQfLxL34HRDfGK8lQ4801U7dLolZByCfF2qjqQAKUEU3IlpDTdZGpcTgfESB46MMRKKGAIIaZRljlmEeKNZA7xqo8TnU7pOovkSbxiE3WOc2DUm5PV37fMghCvugMJgCoDU1o3d6MODKCIT2ZgiJVQwBBCTJPLeHgR/tR1YFR5E+GmKF1IOiUkkYGRdyGlPrc3YW9SOoQwczi0DozPk/scGDGFV3FgSnMOjNJGnd6BAdRZJQoYYh0UMIQQ0+QyHj6Qpo3a6XQkDclLu406oQsplHYOjMiaZL5meY1AQtnKijkwIgMzutIHQFVCKrE5MIEUG8X1cHMfErEBd6EvgBBSeojfqK2exAvEhEY4GpFvdsIN8aSZxCuuI2BoDozxElJi67YoIa3f04Xfvb5TfnxycxU+dWJDxvMCaUpIOc6WyTemHBjuQyI2QAFDCDGNx0Q5JpF0k3jFuYdCqgxMJF0btfY65BBvmoCwmTbqxO9ZXeYBALy96yje3nVUftzpANZ9/2I0VvmRidQh3tJyJ5Q26swOjCsHwUtIKihgCCGmUXYQWTuJF0geZpd2lUDCdaSbxGtqDkxEv4R05RnjcKB7CL1DIfmxl/9xCP2BMPZ3DRkTMKkyMCUmYJQQb2YHhhupiR1QwBBCTJNLpkG0C6dyYNQLHSNRSR76pl9C0l5HUHZ30s2BMdJGLRwY7fdsqPLh3sumaR77/H+9gff3duNwXyDjeQF1CUmbgSm1OTDpskyJyKFvtlETCzEV4l22bBnOOussVFVVobGxEZdffjm2bt2qOWZ4eBhtbW0YPXo0KisrceWVV6Kjo0NzjMPhSPp44oknNMesWrUKZ5xxBnw+H0444QQsX748u1dICLEcTy5t1JkyMKrlhmqBpL/MUXsdoTRD7+QMjAGhIO9B0pk9k0h9PIxrVsDUJbVRl1oGxtgkXkA7oJAQqzAlYFavXo22tjasW7cOK1asQCgUwrx58zAwMCAfs3jxYvztb3/DU089hdWrV+PAgQO44oorks716KOP4uDBg/LH5ZdfLj+3a9cuLFy4EBdeeCE2bNiARYsW4Zvf/CZefPHF7F8pIcQy1KFMSTJ3U8r0m7s6bKsZ6Z9uEm9cuKQP8Rp3jYRT4NIRTYk0VMUEzJH+zAImGpXQNRgrP9VXlnYJKd1Oq0TYhUTswFQJ6YUXXtB8vnz5cjQ2NmL9+vWYM2cOenp68Pvf/x6PP/44LrroIgAxoXLyySdj3bp1OOecc+Svra2tRXNzs+73efjhh9Ha2opf/OIXAICTTz4Za9aswQMPPID58+ebeoGEEOvRzGuJSLolm1RkKiFpBIzagdFxQxKzOHIbte6QPPNdSHplq0QaTDgwPUMh2YUYJTIwJbpKIGDCgXE5te3uhFhBTnNgenp6AAB1dXUAgPXr1yMUCmHu3LnyMVOmTEFLSwvWrl2r+dq2tjbU19fj7LPPxiOPPKL5LW7t2rWacwDA/Pnzk85BCCkM6nKO2U6kjCHe+LmD4aima8Wlt0ogsQvJQAnJSAYmnCY4nEh9lXEBIwK81X63fD2l2IUUiUppJyonwhISsYOsQ7zRaBSLFi3Ceeedh2nTYqG29vZ2eL1e1NbWao5tampCe3u7/Pm9996Liy66COXl5XjppZfwL//yL+jv78e3vvUt+TxNTU1J5+jt7cXQ0BDKysqSricQCCAQUP4B6e3tzfalEUIyoNkwbPK3anMlJCFIYlm5RBLnwKQTR9k4MC4TDoyREpJooRZD7IDSnAOjdovMODAsIREryVrAtLW1YdOmTVizZo3pr/3hD38o///pp5+OgYEB/PznP5cFTDYsW7YM99xzT9ZfTwgxjtrhMLvQ0cggOyDmlKQaKCdwu1I5MMliR5S5jFyvWjhloqEqVgo6bEDAJAZ4gdLMwKh3QRlzYNhGTawnqxLSrbfeiueeew4rV67EuHHj5Mebm5sRDAbR3d2tOb6joyNl3gUAZs2ahf3798sOSnNzc1LnUkdHB6qrq3XdFwBYunQpenp65I99+/Zl89IIIQZQbxg2e1PKOMhO3hodTbtGAFB1IQkHxtAkXgMlpDQbsBNpqIzNfjncF8gYaO5MmMILlGYbtXgP3U5HUqu5HkobNQUMsQ5TAkaSJNx666145pln8Oqrr6K1tVXz/MyZM+HxePDKK6/Ij23duhV79+7F7NmzU553w4YNGDVqFHy+mK06e/ZszTkAYMWKFWnP4fP5UF1drfkghNiHEA9mb0qZulfUA+fkNQIpbpKp5sCkC/EGzUziNVBCqo87MMOhKAaC6ctA8gyYymQBU0pt1GaG2AHqDEzpiDRS/JgqIbW1teHxxx/Hs88+i6qqKjnXUlNTg7KyMtTU1OCGG27A7bffjrq6OlRXV+O2227D7Nmz5Q6kv/3tb+jo6MA555wDv9+PFStW4Cc/+Qm++93vyt/n5ptvxq9//Wt873vfw/XXX49XX30Vf/7zn/H8889b+NIJIbngdjkQjORQQkoxgl6dVQmlmIirHCtKSCbmwBhpo87g/Kgp97pR4XVhIBjB4b4AKn2xf1aD4SiuX/4OPmrvk48dCIQBaEtI3ixCvBv2dWPxkxvQNxzWPH7quBr87utnwmnAOdJDkiQ88PI2PPH2Xqh1qcsJ3HL+JFx3XqvmWo2UjwAkLegk6dl9ZAA3/e+7ODoQa7m/7LSx+OFnphb4qooPUwLmoYceAgBccMEFmscfffRRXHfddQCABx54AE6nE1deeSUCgQDmz5+P//qv/5KP9Xg8ePDBB7F48WJIkoQTTjgB999/P2688Ub5mNbWVjz//PNYvHgxfvnLX2LcuHH43e9+xxZqQoqIbG9KcgnJpf/bu9ednIFJ6cA4lXJT7GtS31i9WQyyM1IeAWKzYAY6B3G4L4DW+goAwAf7u7Fm+xHd42eMq5X/P5sMzF/e/wS7jgwkPf7qR4ewr2sQE0ZXGD6Xmsfe2ov/fGWb7nP/u26PLGDMOjCuhFIfSc/qjw/j445++fM/vb2XAkYHUwLGyMAqv9+PBx98EA8++KDu8wsWLMCCBQsynueCCy7A+++/b+byCCF5RJnGq9x4JUlKEjQup0PTtWTGgcnUziyXkIQDE04teDxu44LLTAYGiAmY3Z2Dmk4k4bzMaq3DPZedIj9e5ffguFoly5fNHJgdh2M3t9s/fRLmnRLr2Lzqt+vQNRjCYIYyViKRaGxlw8ZPunHP3zYDAL598Ym4ZHost9jRG8C1j7yN/V1DiEYlOJ0O0w6MEuJlCckI/XGnbuaEUVi/p4vCLwXchUQIyQq5Ayj+j+twKILP/moNth3q1xzndTvxm6/NxIWTGwGoHZgMAkY1ByZ1CUn8Zq91YNLPgYlCkiTdtmyBvEDSoIDRWyfwcVzAnNZSiynNqTN54udgJgOz83DMfTl30mj53JV+N7oGQxgy0Y69/VA/vvDwm+geVJZTXjKtGYvmnij/fCY1ROF0xN63w/0BNFX7ZQfGZ9iBYQnJDKI0OLa2DOv3dHGHVApyGmRHCBm5yOWbuGjYsK87SbwAMWfhoVU75M/lEG8KB0YT4s1YQtKKqGCaDidN63eG4HEkQ3g4kQadYXZb4wJmSnNV2q/1e8xlYAaDYXzSPQQAmNRQKT9e7on9PjpkwoF5b0+XRrycOWEUfv7FGRpx53E5MaYm5hjtOzqouVZ/ivcwEQ6yM0d/IPaejCr3AAAkKbaGgmihA0MIyYrEAO2mT2KTuS+e0ogHvnIaAOBIXwBz71+Nt3cdxc7D/Ti+oVJVfsg8ByaUoYQkuzUG5sCoHwtFomnFibKN2mAJKWGYnSRJ+Kg9NkxzclP6jkjxczBaQhLuS12FV15HAAB+b+w8ZkpIw3Ex+empTfjFl2agyufWdaZa6srxSfcQ9h4dxJkT6xQHxmAJiduozdEfd2BqyzzyY6FoFD6nMcdrpEAHhhCSFe6Erp7NB2I37Bnja1Ht96Da78HxDZVy6ejJd2KzmYzOgQmGo8YH2SXMgUk3iRdQsjKpECUpvfUFeiSuE+joDaB3OAyX04FJjekDtWZXCYj8y6QG7XnL4+UcMyUkIUSqfG5U+z0py2rj64QDM6T5OuNt1PEMDEtIhhAZmJpyRaDSvUqGAoYQkhWJ5ZuNcQdm2nFax+HLZ40HAPy/9/YjGI5m3IWkt8wx1URcIWzC8a3YosNId3O1OkicoZVanj9jYA4MoFroGHdghPvSWl+R0mkSCCEXiUqGWtJ3xMt0JzRWah4vjzswQ8Fw0tekYjiUPlAtaKkrBwDsTSghGW+jzm5m0EilT8+BofhLggKGEJIVbtV4+MFgWHYGpo2t0Rx30ZRGNFb5cKQ/iP9+faf8eCoHRp2BCWUYKOfRLJWU5EWNegLG4XAordSZBIzJEpJwYI7EHRiRf5mcIf8CaEtpRlyYHfESkjr/AiglJDMZGGWoYHqRNT4uYPZ1xQSM+TZq42sciOLAjKpQBAwdmGQoYAghWaEO8f7jYC8kCWis8qGx2q89zuXEF2bGVo78/MWt8uOZHJhgJCo7KqnbqNU7mSQE4zfklOWphJJTKsImu5DkEG9/bJ3A1o64gGnKLGDU12okB6OUkBIcmLiYGDRVQhJhXIMCJksHhruQzCEGHsbKerHHKP6SYYiXEJIV6hDvpk9iJZNpx9XoHnvdeRPx3t4ueZT+hZMbM4Z4QxFJtVQxfRcSEAs5Cps9ZYu22wkEIxlLSEqI19gNuj6+GiAUkdAzFDLlwLicDnhcDoQiUkYHJhKVsPOIvgNTFndghs2EeGUnJf3rHD8qJmDae4cRCEfkzdlmB9mxDGIM4cBU+t3wOJ0IqtZqEAUKGEJIVqgdGNGBNG2sfsdNY5UfT9yUepeZGu0yR2NzYIC4AxPJEBA2WEKKZOh+SsTndqHa70bvcBjtvcNyO7kRBwaICa5QJJJxFsz+rkEEw1H43E4cN0q72LYsmy4kkYHJUEKqr/SizOPCUCiCT7qGOMjOZkQGptLnjpXfIpxirAdLSISQrFB3ACkBXn0Hxgw+3RCv/j9V6q3YwXA04/wWoxmYTMJJD1FGend3F4LhKMo8Ljn8mgkxEC6TAyPKR631FUkdUmVZlJCEYMrkwDgcDvm17OsayjoDwxBvZoLhqPznoMrnUf6eUfwlQQFDCMkKIRIGgmHZcbBCwIiR/2rbPJ0TIjqFBlXdN6m6ljwJ26tToWRgjP8TKQTMvc9tAQCc1FRpeKmiT9U6no4dh+Llo4QOJEDpQjJXQjKWgQGUVuq9RwdVg+xMbqOmi5ARkX8BgAqfS+n2o/hLgiUkQkhWiH9Yn3n/E0SiEuoqvBhT48/wVZnRbqNO34UEKFux1aWTTCWkYIY5MIqTY9yBOWtiHdbtPCqLkPNPajD8tcosGH3xsXZHJ97ZfRSvfnQIQHL+BQDKvLF/zs2UkIw6MIA2yGt2kJ2bg+wMI/IvZR4X3C6nnMNiCSkZChhCSFZU+WMtnu/v7QYAnDquJu1+IaNoQrwZ5sAAys1RfeNONb/FaAZGCCeXCQfm9k+fhC/OHI9gJAqvyyk7FkYQgisQSr6uYDiK65e/oxlQp5etKcthkF2mDAygBHn3HR2E2OtrdhcSb8KZEfmXCl/s9uxxsoSUCgoYQkhWtF04CVV+NwLhKDwuB756zgRLzut1KeWUkJESkktbQnI7HSlLN4ZLSAaEUyIOhwMto41lXhIRAkIvA9M9FMRQKAKHA/jKWS1oqPLh01Obko4rz2oOjPGdRuphdo3xcpnxEC93IRlFODBV/tjt2eViCSkVFDCEkKw4vqESd3/uFMvPqzeJN1MJCVBu3KnKR4nnToecvTER4s2FdOsEeodii/2q/R4su2J6ynMoIV4zk3jjJSQjDkxcwOw43C/vfDId4uUsk4yIRY6VsgPDElIqKGAIIUWFcD3UId70JSQRJo7Ej80sYIKZBtmJEpLBOTC54k2TgRHbomvLPUnPqSnLwoFRVglkFiItdeXwuZ0YDkUxHIoJmHrVMsl0cJCdcfoDsfdPCBgXS0gpoYAhhBQV8hyYSFT+jT3dQDmP7MDEnIe0Doxqxkw65AF6ReDA9MQdmJqyDALGk42AMR7GLfO68Og3zsJ7e7oAAE3Vfpxz/GhD38fFXUiGEZuoK+MlJIZ4U0MBQwgpKuRZLWFJ/kfbiKsiHJhUU3hjzxltozY3iTdXRAZGr41aODCZBIycgTE1B8ZcO/S5k+px7qR6w+cXcJCdcUQJqSruwIgyJt2rZChgCCFFhSYDY8AJccshXlFCyhz4zeQEhLMYZJcLYht0Tg5MVpN4jbdR54K7hFcJ9AfCeG9PF9RX7nY6MHPCKMPCz9T3S3JgmB9KBQUMIaSoECWgoHoOjFUlJJexEpJSusqPgBGukW4GxmQJKRCfSJw4qTcRSZJMOzDZ4iphF+Gm/3kXb+7oTHr88tPG4v/3ldMt/359AWWNAEAHJh0UMISQokLd6mxmDoyZEG/mXUiZB+hZiezA6MyBEV1ImUK85V7ln/PhUESeI5IKtdtjtB06W5Qt4KXnIuzpjG3gPr6+An6PC32BEPYdHcLu+ONWk+TAMD+UEgoYQkhR4VUNsgsZaGcW7syQAQHjdRuz4418XyuRMzA619U9GNvgncmBUYuQwWBmATOsysrky4EpxRKS+LPy66vPwNSx1Xjt48P4+iNvZ1z7kC39iQ4M80Mp4S4kQkhRoR56Jm4SRkpIYoeMkRJS5jbq/JaQfGkm8YoMTG1Z+pZlp9Mhl5GGDQR5hQPjcjrSij4rKOVBdkLACPGbruXdCpIETAmLP7uhgCGEFBUelQAZMhDMFRa76L5J14VktoRk941dkHYOjBhkl8GBAcwFeZUhdva/RlcJ70IKJXTCpWt5twKxSkCZA1O64s9uKGAIIUWFWqzsPBzbcp0uiyKO3xHfiG1liDdTENYq0q0S6DGYgQFU03iDmafxmhlilyulPMhOuICKgEnd8m4FwkkUGZhSzg/ZDQUMIaSo8DidclvvgZ5hAMpeGD2q40slxbHVaY41vAspi23UuSB+q9e7KfYYnAMDmJsFk18HpjSHsUmSJOeSZAGTpuXdCuRdSL7Y+61M4i2tn10+YIiXEFJUOJ0O/OqqM7Bm22EAQGO1H+dPbkh5/G0Xn4jRlV4Ew1F4XE5ck2appNlVAvnqQkpVQpIkyZwDY2KdQL5aqAElx1Fq4/DVokGUJtO1vFtBYheS+DNbauIvH1DAEEKKjk9PbdLduKxHa30FfrBwqqFjjS9zzHcJSf+3+oFgRL6JGnFg5HUCJhyYdCU3qyjVm7D6z4knHuJVOzCSJMHhsO7PSDQqoT+YmIGhA5MKChhCyIjBcAnJwAoDKxE5lMQuJOG+eF1OWZykI6sQbx4cmHxso97bOSiXXwTj68pQ5c8s/FIRCiuiITEDI0kxUWFlmXEwFIEU/5ZVzMBkhAKGEDJi8LqNOjBiAnCeMzAJ1yVmwFSXeQz9pl9uooQ0LJeQ8uHA2Bviffq9/bj9zx8kPV5f6cWaJRdlLdLU74cog6nn7QTiZUurEOUjl9Mhfx86MKlhiJcQMmLwqIbkpUOeA5OnElKqDIyZ/AuguClGSkiBQjgwNt2E18c3ZFd4XWio8qGhygcAONIfxOG+QNbnVWbAOGUBqW7TD5hYnGkEscix0ueWv5/IYZVafigfUMAQQkYMQsC09wzjlX904GDPkO5xobxvo9YfZGemAwlQHBhDJaS4A2P3GgHA/kF2h+Ii5V8vPRnv/GAu3vnBXNRXxkRMYlnJDLKAUf05cDodqiCvtaIicQYMoApAl1h+KB9QwBBCRgziZr3xkx7c8Id38blfv6F7U5UH2RV4lYAyhdeogInd+AxN4i2AAxOJSpAk62/EQsA0xp0XAKjwxV7XgAUCJjHnkq7tPRcGArH3RD02QIholpCSoYAhhIwY5pzYgE9PbcKM8bVwOR043BfAgW6tCyNJkixg8t6FlODAGN1ELfCbGmQn5sDkYZCdqh3djhvx4d7YDCCNgImLuQEDblQqgmH9MLfXpmm86hKSgCHe1FDAEEJGDDXlHvz318/Es23nYeLocgDKtmGBOh+T9xJSigxMjcEMjJkSUiCPIV51GNrqUogkSTjcH3dgqv3y41Y4MIlD7ASp3q9c6UuYAQMwxJsOChhCyIiktb4CALCrc0DzuLqklL9JvPqrBLpNZmDMLHMUx+RjlYDaybJ6H1LXYEgWnQ2V6hJS3IGxIgOTkBOS294td2CSMzClOkMnH1DAEEJGJBNGxwTM7iNaAaO+weathOTRz1T0mszAmJsDE3dg8hjiBYCIxTfiQ32x8lFdhVcjNCwRMOH8ZmBEG3UVHRhDUMAQQkYkE+MOzJ4EB0b9m64nT6sExA0xHJU0WYfuodgcGLMlJENzYPLowKh1oNUOzKHe5AAvEGupBnLMwKQoIaXbHp4Leg5Mqa5hyAccZEcIGZG0xh2YXQkOjLhROB2xltl8oHYOgpGonL1RupC8hs5jZpVAPnchORwOeFwOhCKS5a3UHfEAb0OigLGkhKQf4k0VujZLMBxF73BI/lxkeSp0BQwdmEQoYAghI5IJ8RDvvqNDiEQlxaqP5HcGDJA4HC2K8rheERmYaltKSHEHJg8lJCBWCglFJMuzHEoLtV/zuNyFZPEcGCB127sZeoZCuPgXq3CkP5j0nMaBkTMwdGASYQmJEDIiGVtbBq/LiWAkqmmlVjZR58d9AWI3KfH91DdFMcjO6CRe2YExtUrAfgcGUMpxVjsJYtJuY3UKByaHEpI8B8atn4HJxYHZfqhfV7zUVXhx7qR6+XO30941DKUMHRhCyIjE5XRgfF0ZdhwewO7OAYyvizkyooSUTwEDxMpI4WBEvimGI1H0xd0D45N4Y/+km9lGnY82akBppbbaSRAh3qQMjBVt1GF9B8aKDIwYJHhSUyVeWnx+yuPcBtdfjETowBBCRiyilXq3ahaMcAjytYlakDhbpHdYufGabaM24sDIGZg8DLIDAJfTnhuxEuJNUULKyYHJkIHJoQtpOCxKeOl//nRgUkMBQwgZsei1UouyQb5aqAWJs2BEgLfC6zIspkQGZigUQTTDDS8gdyHl5zZg10ZqkYFpSiohWbhKIHEOTIq5PWaQ29gz/PyFc5Vpg/pIhCUkQsiIRbRSqwVMOMVv3XYjhMTqjw9jf9eg7ArVlhvrQAKUNmogdnMt86b+7X44j7uQAPVGautuxJIkqUpICQ6MlYPsEh0YjwUOjMGfPx2Y1FDAEEJGLGKdwG7VLJiwvIk6vw6MKP/8/MWtmseNBngB7c1wMBjOIGDyW0KyYyN1XyAsv47UId7sBUwgxSA7ZRt19uUpcd2ZS0jx0hsFTBIUMISQEcvEeAlJ3UodLlAJqe3CE/C/a/cgqtrW7HQ68I1zJxo+h8vpgM/tRCAcxWAwgtFpjhU333yFeGUHxsJSyKH4DJgqvzvJyVDaqC3oQkrlwOTQhWQ0RO2SS28sISVCAUMIGbGoW6kfXLkdlT637Mbkawqv4LMzxuKzM8bmfJ4yrwuBcDTjPiSjDoBV2FEKSTWFF7A4A2PDHJjhsLESktx+zi6kJChgCCEjFpfTgYn15fi4ox/3r/hY81y5Lz83dqsp97jQjVDaYXaSJKluoPkK8Vp/I041xA5QHJhAOIqwarqxGUQXUuIyR68Fc2CMhni5Cyk1FDCEkBHN3Z89BX9+dx/U9wenA/jyWS2Fu6gc8HszrxMIRqIQlap87EICbCohiQBvtZ4Do9zeBoIR1JSZFzDBDMscrZgDkymD5LFpfs6xAAUMIWREc+4J9Tj3hPrMB5YIRhY6qrtn8ufA5LeE5HU75f1LA4Gw4Vk6ajKVkPLRhVRIB2YwGMbvX98lt/RfdHKjZkpwoaGAIYSQY4hyT+yf9QM9QzjSH0B9ZfLNXdw8HY7kFmG7UNqoc78RhyNRHO4PYO/RWKu5XgkJiLkw3YOhrHMwcht10hyY2OfBHASMskwz/c/fjtKbUR5/ay9+oSqt/vWDA3j7B3Pzfh2p4CA7Qgg5hhAlpB88swln/tvLeHj1jqRjAnKA1wmHIz/dVkobdW6lkEhUwiW/fB2zl72Kl7Z0ANAvIQG5T+OVMzA2rBIoBQfm9W1HAABnt9YBALoGk3c3FRIKGEIIOYa4ZFozKrwuuevn1Y8OJR2T7yF2gDoDk9uNuHswiG2H+gHEhMX4ujKcc7x+w3iunUjBlCUkKwbZxUVkpi4kkYHJcxt1MBzF27uOAgAWzT0RQOy9K6aBeiwhEULIMcRVZ7fgqrNbsOmTHnzmV2uw83B/0jHixutz5+93WDGQLdcbYH9cjJR5XPjHjxekPTbXabyhcKo5MPE2agt2IfkzvAfyzy3PJaQN+7oxFIpgdIUXM8bVyo8PhyKagHQhoQNDCCHHIGJR5ZH+ILoTrP9CODBWddMIAVPpz3wTVUpIuTowqbqQ8hfitXIFgxHe2B4rH82eNFqeEg0g43yhfEIBQwghxyAVPjfG1sTCrTsSXJh8rxEArCshicm6lQZcAFFC6s9yGm+qEK81GRgR4s1UQrJ+BYMRhIA574R6OJ0OOQc0nINosxoKGEIIOUaZ1FgJANhxaEDzuNEx9lZi1Y24PxBr6TUkYOIOzGDWJST9xZ4+SwbZGVwloBJ+kpQfETMQCGPDvm4AwHnxtmlxnXRgCCGE2M6khriASXBglAxMARyYHEshwk2pMDApOdcMTOoQb+6rBIy+B+ryVb5MmLd3HUU4KmHcqDK0xBeeCqeomARMcSRxCCGEWI7swCSVkGI3IV9eHZj4ILscS0j9w/EMjAEHRqyDyL6NOkMGJo8OjLgel9Na0TkciuDaR97GriOKSyfWUJynGlpHAUMIISRvTGqIBXm3H0oQMAYXCVqJVYPshJtiRMBUenPsQhIZmJRt1BFIkpTVLB2jIV61+2NHDmbzgR68FW+XTuTSU8fI/6+UkEo0A7Ns2TKcddZZqKqqQmNjIy6//HJs3bpVc8zw8DDa2towevRoVFZW4sorr0RHR4fmmL1792LhwoUoLy9HY2Mj7rjjDoTD2j9gq1atwhlnnAGfz4cTTjgBy5cvz+4VEkLICOWEeAlp79FBTeDUaIDUSpQ2amu6kIy08pb7LBpklzSJN/Zzi0rZD5gzGqRWOzB2TOMVbktrfQX+/q1PyR9v/OtFOP+kBvm4YnRgTAmY1atXo62tDevWrcOKFSsQCoUwb948DAwo1tPixYvxt7/9DU899RRWr16NAwcO4IorrpCfj0QiWLhwIYLBIN5880384Q9/wPLly3HXXXfJx+zatQsLFy7EhRdeiA0bNmDRokX45je/iRdffNGCl0wIISODhiofqvxuRCVgT+eg/LgQM/mcA6O0UVszB8ZIG3VlroPsUs6BcSYdYwYz28Dd6hKSDa3UYmdWbbkHU8dWyx/H1ZZpjhNCq5gcGFMlpBdeeEHz+fLly9HY2Ij169djzpw56Onpwe9//3s8/vjjuOiiiwAAjz76KE4++WSsW7cO55xzDl566SVs2bIFL7/8MpqamnDaaafhxz/+MZYsWYK7774bXq8XDz/8MFpbW/GLX/wCAHDyySdjzZo1eOCBBzB//nyLXjohhBzbOBwOTGqoxIZ93dhxqB8nNVUBUDsw+RMwrrgDk+tIfLmE5DXSRh07pj/HElKigFGXlALhKCr0NxmkxMw2cIfDAZfTgUjUnim4Ymt5WYbr8B1rXUg9PT0AgLq62J6E9evXIxQKYe5cZdnTlClT0NLSgrVr1wIA1q5di+nTp6OpqUk+Zv78+ejt7cXmzZvlY9TnEMeIcxBCCDGG6ERS52ACIn+Rxy4kqwbZ9ZkoIclt1FkOslPmwGgzLk6nQ3492cyCUbsYRkSkW26ltt79GDYoYMTzwznMvrGarEO80WgUixYtwnnnnYdp06YBANrb2+H1elFbW6s5tqmpCe3t7fIxavEinhfPpTumt7cXQ0NDKCvTWlsAEAgEEAgE5M97e3uzfWmEEHLMMKkxFuRVdyIVdBeSVQ6MkUm8cht1bhmYRAcGiOVgQpFwVp1IAZPbwN1OBwKwJ8QrSkhiCWgqlAxMiZaQ1LS1tWHTpk1Ys2aNldeTNcuWLcM999xT6MsghJCiQgR5V398GDf+z7sAgC0HYr/g5TcDY81OHzNdSOVeezIwQOxn1x/IbhaMOsBrpIPJ7XICiBieYrz7yAAeePljOaArOGviKNw0Z5LmsaH4tWRyYIpxkF1WAubWW2/Fc889h9deew3jxo2TH29ubkYwGER3d7fGheno6EBzc7N8zNtvv605n+hSUh+T2LnU0dGB6upqXfcFAJYuXYrbb79d/ry3txfjx4/P5uURQsgxw9Sx1XA6gK7BEFZs0f67OqZW/99TO7BqkF2fiTkwlTkMspMkKeUgO0C1TiAbB8ZggFcgSkhGHZjfvLYDz244kPT4ii0d+NKZ41Fb7pUfM5qBKcYuJFMCRpIk3HbbbXjmmWewatUqtLa2ap6fOXMmPB4PXnnlFVx55ZUAgK1bt2Lv3r2YPXs2AGD27Nn493//dxw6dAiNjY0AgBUrVqC6uhpTp06Vj/n73/+uOfeKFSvkc+jh8/ng85lMUhFCyDHOuFHleOKm2UnD7GrLPJg7tSnFV1mP2ZtwKsRiRmNt1LGb7mAogmhUgtNpfF6LOmysV+bx5bAPyWwbu9tlLgOzfk8XAOD681pxYlPMgfvRs5sRjETRNxzWCBg5A2O4hFSiAqatrQ2PP/44nn32WVRVVcmZlZqaGpSVlaGmpgY33HADbr/9dtTV1aG6uhq33XYbZs+ejXPOOQcAMG/ePEydOhVf+9rXcN9996G9vR133nkn2traZAFy880349e//jW+973v4frrr8err76KP//5z3j++ectfvmEEHLsc3ZrHc5urSvoNQgBk3MbdRYOjCTFnAYjokegFgsed7LwkdcJZNFGbXaQoDJDJ/PPrmcohG3xwPYtF0xCQ1XsvvofL25F50AwqawkZ2AyOTDuEh9k99BDD6GnpwcXXHABxowZI388+eST8jEPPPAAPvOZz+DKK6/EnDlz0NzcjKefflp+3uVy4bnnnoPL5cLs2bPx1a9+FV//+tdx7733yse0trbi+eefx4oVKzBjxgz84he/wO9+9zu2UBNCSInidok26txugPI2agMh3jKPCyJiYraMJBY5AikyMB7hwGSTgTE3h0c4MEZ+dhv2dUOSgJa6clm8AIrDktiRZbyNusQdGCObMP1+Px588EE8+OCDKY+ZMGFCUokokQsuuADvv/++mcsjhBBSpFgxyC4Qjsi5FCNzYBwOByq8bvQHwqan8arDuW6d0pMoK+WjhOQy4V69Fy8fzZwwSvO40lKe4MDIAia9mFLaqEvUgSGEEEKyQQyyy6WNWt0ObWQbtfo40w6MPAPGqdspZIUDYzTE6zExBPC9vTEBc0ZLreZxxYHRCpjhYOlmYChgCCGE2I68jTqHEpIQIX6PUy5JZaIiy4WOqRY5CkQGJjcBY9KBySBgolEJG/Z2AwBOb0lwYHzpS0gZMzBF2EZNAUMIIcR25DbqHEpISgu1x/DXiOBu12DIVOBWWSOg37mkdCFlE+KNas6RCaNTjLcd6kdfIIxyrwtTmqs0z5V5MpWQ6MAQQgghSZjppEmFaKGuNFg+ApRhdjf/cT1OvusFPPnOXkNfFwynnsILqOfAmL+hB0w6MEoAOv3PTrRPzxhXm+RQpSqlDRkuIZV4FxIhhBCSDUobdfY3QNFCbaYd+oLJjXInUiQqYeVHhw19XbohdkCODozJXVSZQrxPv7cf339mI/5n7W4AyQFeQBFyQ4kZGKMOjLv4HJisVwkQQgghRlFagbN3YPpNrBEQ3HLBJHzjvIl4/sOD+M5TH+DoQNDQ16lDvHrkNAfG5DZwT5o26t7hEL771AdQ/1j1Zv6UiyxQqhJSBgfGdywtcySEEEKMIpyMXNqozexBUuP3uDCm1g8AODIQyHB0jFA4fQbGa4UDYzjEm/pn1zMYQlSKXedtF52I5ho/PnVifdJxigOjLSGJTIzhbdRFVEKigCGEEGI7VuxC6jexiTqR0RWxoW5GHRjjJaQs5sDEv8ZnUMB40qxhEA5Kld+Db118YspzpHJgjK8SYBcSIYSQEYjSRp17CclMBkZQVxHb/9M9GDKUwxHdUqkFTP5KSOnEn3ClyjMIEL0MTCgSlV8nu5AIIYQQHdKVQYxiZg9SIqPKldbrrsFQxuMzzoGxYpCdwRCvEFF64k+UgCoyTCYWAmZAVUJSi5HMc2Biz4ciUs4LOa2CJSRCCCG2I7qQOnqHMf+B19Ie++mpTfju/MlJjytt1OZvXW6XE6PKPegaDOHoQFCzJ0iPTCFeZZVALg6MuS4kvRk6sgOTobW8XGeVgCg/ORyZZ9Ko3aJhk4sx7aLwV0AIIeSYZ2xtGbwuJ4KRKLZ29KU9dmtHH265YFLSTbIvizZqNXUVXnQNhtA5EABQlfbYYIYQr+zAZDMHRt5GbW6Zo94UY8MOjM4k3uFg7HyxpZf6r1OgdosoYAghhIwY6iq8WHnHBdhzZCDtcbf96X10DgSx7VA/Thtfq3lOuA1VWd48R1f4sOPwgKEgr+EMTBZzbQImHRh3OgcmLkgyhXDLPcm7kIxO4QUAp9MhC9BiWehIAUMIISQvHFdbhuNqy9Iec/KYaqzZfgRb23t1BEzcbcjBgQGMdSLJqwRSzoERDkw2qwTMOjCpMzBDsgOTXoSIn9lgIFnAGBVSfk9cwBRJkJchXkIIIUXDSU2x0s5H7cllpj65C8n4KgE1dZUxAXOkP7OAESWkVCFeby5t1CZDvOmmGAtRV55B1CnbqJUSktE1AoJi60SigCGEEFI0iCWEH+vkZOQSUhZzYABgtOzAZB5mF7RzmWPctTE6B0bskdKbYiwESUYHRifEa3SNgIAChhBCCEnB5LiA2arjwOQyBwZQCxgTJSRb5sBkF+LVEzAiA1OeIcQrXJZwVJKv2UwGRn29xTKNlwKGEEJI0XBiUyUcjliZ50i/1inJZheSmrrKWOt0p4ESUkYBY8UcGJMhXr0ZOoNyLsjYIDtAcW1ECcnPEhIhhBCSG+VeN1rqygEAH6tcmGA4KjsH2QoYcw5MTCxkngOTzSqBeAkpw+wVgSxg0rRRl2VwYDwup3zN4msUB8bYdSgbqenAEEIIIUlM1gnyivwLkJ8upExzYPxZOjBRVQnHsAPjSp2BGTCYgQGSg7xmMzC+ItuHRAFDCCGkqNAL8orykc/tTFnWyYRwYLoGgxnH4SurBPRv7tlmYNSCx+wkXr0uJOGmZMrAAIrIkR0Yk11I8kbqLFwnO6CAIYQQUlRMbq4GoHVgcs2/AMCouICJSkD3YHoXRpkDo+/AiNLScCiClR8dwuYDPYauQbN/yGAJSbhAehmYAROt5UKoiNZr83NgWEIihBBCUjK5uRJAzIGJxp0ScaOuzLKFGojlQGrKYksdM5WR5AxMCrdH3MyjEvCN5e9g4X+uwYf7uzNeg3Av3E6HXBrKhCttG7UJByYu/oZC8RBv1l1IxeHAcBIvIYSQomLi6Ap43U4MBiO48Ber4HI4DO/8ycToCi96hkLoHAjixDTHBTN0IdWUefDPc47H2p2d2N81hKMDQazb2YlTx9Wm/f5mFznGriFdiNeEA+PROjBCiJSzC4kQQgjJHbfLibMn1gEA9nQOYueRAbT3DgMATmqqzOncRoO8oXB6AQMASy89GX+99Z9wwz+1AgA2ftKb8fubnQEDqAbZ6bVRCwfGY8KBiX+N+FrzJaTiEDB0YAghhBQd//31M7H5QA/Ut2ynw4Hpx9XkdF4hYDozCJhMk3jVTItf0+ZPMudgxM3fZ3CNAKBuo9YKmGhUUgSMmQxMwhwYw6sE3MU1yI4ChhBCSNFR5nXhzLgLYyWj4/uQjmYYZid3IRkI2k4bGwsd7zwygL7hEKr8npTHKiUkEw5Mikm8QyonxEhpLakLyXQbdXE5MCwhEUIIGTGMrohN4820DykUjokFIy3boyt9GFvjBwBsOZC+jBQImyvbAKnbqIWT4nAYE0Tl8j6k7ObAKG3UxeHAUMAQQggZMYgS0hHDJSRjt8lT4mWkTRkETHYhXv0uJHmNgNcNhyNzqas8hQPDVQKEEEJIkWO2hGQkAwMA08Yay8EoDozx228mB8ZohkUWMGIOTJBt1IQQQkhJIByYLQd7sfjJDfLjNWUeLJp7ImrLY88rk3iNCY3p42I5mI0ZBIzchWQixCtEVOL04CG5tdyogImXkEKijTr2Go0LmOJyYChgCCGEjBjEosieoRCeef8TzXPNNX7cfP4kAJmXOSYiHJgdh/sxGAwnDZYLRaLY1tGPXUcGAZjNwDg11yQYMDHELnaccGASBtkZLiGxC4kQQggpCBNGV+CR687EzsMD8mPv7D6KFzd3YON+xT0JGpgDo6ax2o+GKh8O9wXwj4N9mDlhlPzcWzs7sfSZjZrv6TNRQvI49R2YQRNrBACg3CdCvFmWkNx0YAghhJCCcdGUJlw0Rfl8SnM1XtzcgU2qfUYhkyFeAJh+XA1e/egQvvf//wCjK33yed7f2w0gVuqpLvPA73HhstOOM3xekYEJRRMzMCYdGI+yjVqSJNO7kHxFtsyRAoYQQsiIZtpxsfzKns5B9AyFUFPmUc2BMRbiBYBZrXV49aND2HF4ADtUbgsAXD2rBUsWTJF3MZlB7ExKcmBMrBEAlGF3g8GIZiu26W3ULCERQgghhae23Itxo8qwv2sImw/04NxJ9aZLSADwjfNacVJzldzlIzi+oQInj6nO+vrkSbyJGZiAKAEZzcAoJSRRPgKMb8VmFxIhhBBSZEwbWxMTMJ/04txJ9XJg1oyA8bqduHByo+XX5k6xzHHIpAOjTOINy+Ujr8tpeCt2sXUhcQ4MIYSQEY8oI2060ANJkkwPsrOTVMsczWZglF1IEVX+xfjrEwImFJGSylmFoPDvDCGEEFJgxCTdjZ/0aCbeGp0DYyepdiHJGRiDGRaxLykYjqJ/2NwQPEArdorBhSn8O0MIIYQUGDHHZdeRAXQNKlN6PSZCvHbhTjWJNyA2UZtzYADgaHyVgtEWakA7fI8ChhBCCCkCGqp8aK72Q5KAD/cp7dRFUUJKtQvJpAPjczvlluxOIWAMlp8AwOl0yI5UMSx0LPw7QwghhBQBIgfzxo4jAGJbnoX7UUhkByZJwJibpOtwOORZMJ39sW3cZSYyMIBSRvqkawgdvcPojw/TKwQUMIQQQgiAU+JlpEff2A0A8DidhrY8241bNYlXkhQRMxBUtlEbRcyCURwY4yUkQAnyfuk3azHrJ6/gf9buNvX1VkIBQwghhAC4dPoY1Fd64XI64HI68JlTxxT6kgAoXUiA1oURqwTKDbZRA0rH0hHZgTEnYBaeOgbu+M/H5XTAgcIJPM6BIYQQQgBMbq7Cu3d+utCXkYToQgJiLozQHINZODBCsOztNL9UEgB+9NlT8KPPnmLqa+yCDgwhhBBSxKgFTEjViTRgcpAdAFTGO5be3dMFwLwDU0zQgSGEEEKKGHUJKaIpIYkQr/Fb+VWzxuNwfwDhaBR+twufP934UsligwKGEEIIKWJcTgccDkCSIK84CEWi8rRgo23UAPD508fh86ePs+U68w1LSIQQQkiRo+5EApT8C2B8lcCxBgUMIYQQUuSIMpLIwIghdh6XA16D26SPNUbmqyaEEEJKiEQHRl4jMELdF4AChhBCCCl6lIWOWgfGTP7lWIMChhBCCClyXHIJSevAmJ2keywxcr0nQgghpETwxB2Yh1btwOhKL/Z3DQEAKgxuoj4WGbmvnBBCCCkRaso8ONgzjL9+cEDzeH2lr0BXVHgoYAghhJAi5+dfmIEXN7dDgjLIzuV04vLTxhbwqgoLBQwhhBBS5EwfV4Pp42oKfRlFBUO8hBBCCCk5KGAIIYQQUnJQwBBCCCGk5KCAIYQQQkjJQQFDCCGEkJLDtIB57bXX8NnPfhZjx46Fw+HAX/7yF83zHR0duO666zB27FiUl5djwYIF2LZtm+aYCy64AA6HQ/Nx8803a47Zu3cvFi5ciPLycjQ2NuKOO+5AOBw2/woJIYQQcsxhWsAMDAxgxowZePDBB5OekyQJl19+OXbu3Ilnn30W77//PiZMmIC5c+diYGBAc+yNN96IgwcPyh/33Xef/FwkEsHChQsRDAbx5ptv4g9/+AOWL1+Ou+66K4uXSAghhJBjDdNzYC655BJccsklus9t27YN69atw6ZNm3DKKacAAB566CE0NzfjT3/6E775zW/Kx5aXl6O5uVn3PC+99BK2bNmCl19+GU1NTTjttNPw4x//GEuWLMHdd98Nr9dr9rIJIYQQcgxhaQYmEAgAAPx+v/INnE74fD6sWbNGc+xjjz2G+vp6TJs2DUuXLsXg4KD83Nq1azF9+nQ0NTXJj82fPx+9vb3YvHlzyu/d29ur+SCEEELIsYmlAmbKlCloaWnB0qVL0dXVhWAwiJ/97GfYv38/Dh48KB939dVX449//CNWrlyJpUuX4n//93/x1a9+VX6+vb1dI14AyJ+3t7frfu9ly5ahpqZG/hg/fryVL40QQgghRYSlqwQ8Hg+efvpp3HDDDairq4PL5cLcuXNxySWXQJKU/Q033XST/P/Tp0/HmDFjcPHFF2PHjh2YNGlSVt976dKluP322+XPe3t7KWIIIYSQYxTL26hnzpyJDRs2oLu7GwcPHsQLL7yAzs5OHH/88Sm/ZtasWQCA7du3AwCam5vR0dGhOUZ8nio34/P5UF1drfkghBBCyLGJbXNgampq0NDQgG3btuHdd9/FZZddlvLYDRs2AADGjBkDAJg9ezY2btyIQ4cOycesWLEC1dXVmDp1ql2XTAghhJASwXQJqb+/X3ZKAGDXrl3YsGED6urq0NLSgqeeegoNDQ1oaWnBxo0b8e1vfxuXX3455s2bBwDYsWMHHn/8cVx66aUYPXo0PvzwQyxevBhz5szBqaeeCgCYN28epk6diq997Wu477770N7ejjvvvBNtbW3w+XyGrlOUrBjmJYQQQkoHcd9WR090kUyycuVKCUDSx7XXXitJkiT98pe/lMaNGyd5PB6ppaVFuvPOO6VAICB//d69e6U5c+ZIdXV1ks/nk0444QTpjjvukHp6ejTfZ/fu3dIll1wilZWVSfX19dJ3vvMdKRQKGb7Offv26V4nP/jBD37wgx/8KP6Pffv2pb3POyQpk8QpTaLRKA4cOICqqio4HI6czydCwfv27Ttm8zV8jaXPsf76gGP/NR7rrw/gazwWsPP1SZKEvr4+jB07Fk5n6qSLpV1IxYTT6cS4ceMsP+9ICAjzNZY+x/rrA47913isvz6Ar/FYwK7XV1NTk/EYLnMkhBBCSMlBAUMIIYSQkoMCxiA+nw8/+tGPDHdBlSJ8jaXPsf76gGP/NR7rrw/gazwWKIbXd8yGeAkhhBBy7EIHhhBCCCElBwUMIYQQQkoOChhCCCGElBwUMIQQQggpOShgDPLggw9i4sSJ8Pv9mDVrFt5+++1CX1JWLFu2DGeddRaqqqrQ2NiIyy+/HFu3btUcc8EFF8DhcGg+br755gJdsXnuvvvupOufMmWK/Pzw8DDa2towevRoVFZW4sorr0zafl7sTJw4Mek1OhwOtLW1ASi99/C1117DZz/7WYwdOxYOhwN/+ctfNM9LkoS77roLY8aMQVlZGebOnYtt27Zpjjl69CiuueYaVFdXo7a2FjfccAP6+/vz+CrSk+41hkIhLFmyBNOnT0dFRQXGjh2Lr3/96zhw4IDmHHrv+09/+tM8v5LUZHofr7vuuqTrX7BggeaYYn4fM70+vb+TDocDP//5z+Vjivk9NHJ/MPLv5969e7Fw4UKUl5ejsbERd9xxB8LhsOXXSwFjgCeffBK33347fvSjH+G9997DjBkzMH/+fM227FJh9erVaGtrw7p167BixQqEQiHMmzcPAwMDmuNuvPFGHDx4UP647777CnTF2XHKKadorn/NmjXyc4sXL8bf/vY3PPXUU1i9ejUOHDiAK664ooBXa5533nlH8/pWrFgBAPjiF78oH1NK7+HAwABmzJiBBx98UPf5++67D//5n/+Jhx9+GG+99RYqKiowf/58DA8Py8dcc8012Lx5M1asWIHnnnsOr732Gm666aZ8vYSMpHuNg4ODeO+99/DDH/4Q7733Hp5++mls3boVn/vc55KOvffeezXv62233ZaPyzdEpvcRABYsWKC5/j/96U+a54v5fcz0+tSv6+DBg3jkkUfgcDhw5ZVXao4r1vfQyP0h07+fkUgECxcuRDAYxJtvvok//OEPWL58Oe666y7rL9jwdsQRzNlnny21tbXJn0ciEWns2LHSsmXLCnhV1nDo0CEJgLR69Wr5sfPPP1/69re/XbiLypEf/ehH0owZM3Sf6+7uljwej/TUU0/Jj/3jH/+QAEhr167N0xVaz7e//W1p0qRJUjQalSSptN9DANIzzzwjfx6NRqXm5mbp5z//ufxYd3e35PP5pD/96U+SJEnSli1bJADSO++8Ix/zf//3f5LD4ZA++eSTvF27URJfox5vv/22BEDas2eP/NiECROkBx54wN6Lswi913jttddKl112WcqvKaX30ch7eNlll0kXXXSR5rFSeg8T7w9G/v38+9//LjmdTqm9vV0+5qGHHpKqq6s1i52tgA5MBoLBINavX4+5c+fKjzmdTsydOxdr164t4JVZQ09PDwCgrq5O8/hjjz2G+vp6TJs2DUuXLsXg4GAhLi9rtm3bhrFjx+L444/HNddcg7179wIA1q9fj1AopHk/p0yZgpaWlpJ9P4PBIP74xz/i+uuv1ywuLfX3ULBr1y60t7dr3rOamhrMmjVLfs/Wrl2L2tpanHnmmfIxc+fOhdPpxFtvvZX3a7aCnp4eOBwO1NbWah7/6U9/itGjR+P000/Hz3/+c1useTtZtWoVGhsbMXnyZNxyyy3o7OyUnzuW3seOjg48//zzuOGGG5KeK5X3MPH+YOTfz7Vr12L69OloamqSj5k/fz56e3uxefNmS6/vmF3maBVHjhxBJBLRvBkA0NTUhI8++qhAV2UN0WgUixYtwnnnnYdp06bJj1999dWYMGECxo4diw8//BBLlizB1q1b8fTTTxfwao0za9YsLF++HJMnT8bBgwdxzz334FOf+hQ2bdqE9vZ2eL3epJtCU1MT2tvbC3PBOfKXv/wF3d3duO666+THSv09VCPeF72/g+K59vZ2NDY2ap53u92oq6sryfd1eHgYS5YswVVXXaVZlPetb30LZ5xxBurq6vDmm29i6dKlOHjwIO6///4CXq1xFixYgCuuuAKtra3YsWMHvv/97+OSSy7B2rVr4XK5jqn38Q9/+AOqqqqSytOl8h7q3R+M/PvZ3t6u+3dVPGclFDAjmLa2NmzatEmTDwGgqTdPnz4dY8aMwcUXX4wdO3Zg0qRJ+b5M01xyySXy/5966qmYNWsWJkyYgD//+c8oKysr4JXZw+9//3tccsklGDt2rPxYqb+HI5lQKIQvfelLkCQJDz30kOa522+/Xf7/U089FV6vF//8z/+MZcuWlcTI+q985Svy/0+fPh2nnnoqJk2ahFWrVuHiiy8u4JVZzyOPPIJrrrkGfr9f83ipvIep7g/FBEtIGaivr4fL5UpKWXd0dKC5ublAV5U7t956K5577jmsXLkS48aNS3vsrFmzAADbt2/Px6VZTm1tLU466SRs374dzc3NCAaD6O7u1hxTqu/nnj178PLLL+Ob3/xm2uNK+T0U70u6v4PNzc1JofpwOIyjR4+W1PsqxMuePXuwYsUKjfuix6xZsxAOh7F79+78XKDFHH/88aivr5f/XB4r7+Prr7+OrVu3Zvx7CRTne5jq/mDk38/m5mbdv6viOSuhgMmA1+vFzJkz8corr8iPRaNRvPLKK5g9e3YBryw7JEnCrbfeimeeeQavvvoqWltbM37Nhg0bAABjxoyx+ersob+/Hzt27MCYMWMwc+ZMeDwezfu5detW7N27tyTfz0cffRSNjY1YuHBh2uNK+T1sbW1Fc3Oz5j3r7e3FW2+9Jb9ns2fPRnd3N9avXy8f8+qrryIajcrirdgR4mXbtm14+eWXMXr06Ixfs2HDBjidzqSyS6mwf/9+dHZ2yn8uj4X3EYi5ojNnzsSMGTMyHltM72Gm+4ORfz9nz56NjRs3aoSoEONTp061/IJJBp544gnJ5/NJy5cvl7Zs2SLddNNNUm1trSZlXSrccsstUk1NjbRq1Srp4MGD8sfg4KAkSZK0fft26d5775XeffddadeuXdKzzz4rHX/88dKcOXMKfOXG+c53viOtWrVK2rVrl/TGG29Ic+fOlerr66VDhw5JkiRJN998s9TS0iK9+uqr0rvvvivNnj1bmj17doGv2jyRSERqaWmRlixZonm8FN/Dvr4+6f3335fef/99CYB0//33S++//77cgfPTn/5Uqq2tlZ599lnpww8/lC677DKptbVVGhoaks+xYMEC6fTTT5feeustac2aNdKJJ54oXXXVVYV6SUmke43BYFD63Oc+J40bN07asGGD5u+m6Nx48803pQceeEDasGGDtGPHDumPf/yj1NDQIH39618v8CtTSPca+/r6pO9+97vS2rVrpV27dkkvv/yydMYZZ0gnnniiNDw8LJ+jmN/HTH9OJUmSenp6pPLycumhhx5K+vpifw8z3R8kKfO/n+FwWJo2bZo0b948acOGDdILL7wgNTQ0SEuXLrX8eilgDPKrX/1Kamlpkbxer3T22WdL69atK/QlZQUA3Y9HH31UkiRJ2rt3rzRnzhyprq5O8vl80gknnCDdcccdUk9PT2Ev3ARf/vKXpTFjxkher1c67rjjpC9/+cvS9u3b5eeHhoakf/mXf5FGjRollZeXS5///OelgwcPFvCKs+PFF1+UAEhbt27VPF6K7+HKlSt1/1xee+21kiTFWql/+MMfSk1NTZLP55MuvvjipNfd2dkpXXXVVVJlZaVUXV0tfeMb35D6+voK8Gr0Sfcad+3alfLv5sqVKyVJkqT169dLs2bNkmpqaiS/3y+dfPLJ0k9+8hPNzb/QpHuNg4OD0rx586SGhgbJ4/FIEyZMkG688cakXwSL+X3M9OdUkiTpN7/5jVRWViZ1d3cnfX2xv4eZ7g+SZOzfz927d0uXXHKJVFZWJtXX10vf+c53pFAoZPn1OuIXTQghhBBSMjADQwghhJCSgwKGEEIIISUHBQwhhBBCSg4KGEIIIYSUHBQwhBBCCCk5KGAIIYQQUnJQwBBCCCGk5KCAIYQQQkjJQQFDCCGEkJKDAoYQQgghJQcFDCGEEEJKDgoYQgghhJQc/x973vzpvwP0lQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(x_data , y_data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zR-zxdJBJw"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "x_data_normalized = scaler_x.fit_transform(x_data.reshape(-1,1))\n",
        "y_data_normalized = scaler_y.fit_transform(y_data.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data_normalized, y_data_normalized, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piPQbPEYJfpU"
      },
      "outputs": [],
      "source": [
        "# Function to create an LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(10, activation='relu', input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV27U60LJukA"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate the LSTM model with early stopping\n",
        "def train_evaluate_lstm(x_train, y_train, x_test, y_test, epochs=5, batch_size=1):\n",
        "    input_shape = (x_train.shape[1], 1)\n",
        "    model = create_lstm_model(input_shape)\n",
        "\n",
        "    # Implement early stopping\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(x_test, y_test),\n",
        "              callbacks=[early_stopping])\n",
        "\n",
        "    y_pred = model.predict(x_test, batch_size=1, verbose=0)\n",
        "    mse = mean_squared_error(y_test, y_pred.squeeze())\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCovdRsAJyLQ"
      },
      "outputs": [],
      "source": [
        "# PSO Objective Function\n",
        "def objective_function(params, x_train, y_train, x_test, y_test):\n",
        "    epochs, batch_size = params\n",
        "    mse = train_evaluate_lstm(x_train, y_train, x_test, y_test, epochs=int(epochs), batch_size=int(batch_size))\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRrCvIhoKDqA",
        "outputId": "fd6e5d06-7a62-48ab-d713-45fe2bf1d2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0213 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0249 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0264 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0306 - val_loss: 0.0111\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0238 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0221 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0253 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0254 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0267 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0228 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0290 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0241 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0263 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0280 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0144\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0236 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0247 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0119\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0243 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0285 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0236 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0244 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0226 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0281 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0253 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0273 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0223 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0239 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0293 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0235 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0142\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0264 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0241 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0138\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0215 - val_loss: 0.0137\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0256 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0267 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0224 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0264 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0311 - val_loss: 0.0113\n",
            "426/426 [==============================] - 5s 8ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0235 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0263 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0264 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0321 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0242 - val_loss: 0.0133\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0246 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0239 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0241 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0253 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0282 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0228 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0273 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0213 - val_loss: 0.0139\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0261 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0280 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 10ms/step - loss: 0.0237 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0130\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0298 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0212 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0266 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0239 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0272 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0251 - val_loss: 0.0120\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0228 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0235 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0302 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0261 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0243 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0315 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0142\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0237 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0270 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0257 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0285 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0283 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0214 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0229 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0213 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0271 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0235 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0266 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0242 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0251 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0257 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0246 - val_loss: 0.0122\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0222 - val_loss: 0.0149\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0298 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0321 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0249 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0272 - val_loss: 0.0119\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0257 - val_loss: 0.0127\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0241 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0257 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0237 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0257 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0246 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0291 - val_loss: 0.0114\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0254 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0241 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0220 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0262 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0218 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0262 - val_loss: 0.0115\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0234 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0223 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0119\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0223 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0323 - val_loss: 0.0117\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0255 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0225 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0217 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0237 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0251 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0227 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0264 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0238 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0119\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0247 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0236 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0290 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0281 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0310 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0260 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0241 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0243 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0272 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0212 - val_loss: 0.0130\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0263 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0306 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0341 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0231 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0256 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0226 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0243 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0237 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0220 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0272 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0211 - val_loss: 0.0148\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0287 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0285 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0134\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0225 - val_loss: 0.0144\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0221 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0237 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0260 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0249 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0245 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0271 - val_loss: 0.0123\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0234 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0240 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0144\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0215 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0281 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0282 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0294 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0267 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0238 - val_loss: 0.0131\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0231 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0263 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0211 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0316 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0280 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0225 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0229 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0214 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0223 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0284 - val_loss: 0.0114\n",
            "426/426 [==============================] - 5s 8ms/step - loss: 0.0244 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0262 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0240 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0264 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0260 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0236 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0269 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0240 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0224 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0236 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0255 - val_loss: 0.0123\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0300 - val_loss: 0.0111\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0268 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0314 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0230 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 10ms/step - loss: 0.0274 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0241 - val_loss: 0.0122\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0253 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0267 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0208 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0222 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0237 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0216 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0121\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0232 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0209 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0241 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0250 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0220 - val_loss: 0.0150\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0242 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0213 - val_loss: 0.0136\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0229 - val_loss: 0.0130\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0253 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0239 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0234 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0234 - val_loss: 0.0121\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0221 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0277 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0251 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0284 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0269 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0145\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0276 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0264 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0218 - val_loss: 0.0131\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0268 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0131\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0264 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0305 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0146\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0129\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0249 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0254 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0144\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0244 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0245 - val_loss: 0.0117\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0227 - val_loss: 0.0132\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0217 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0226 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0234 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0242 - val_loss: 0.0119\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0224 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0142\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0238 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0125\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0222 - val_loss: 0.0135\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0218 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0254 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0222 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0250 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0287 - val_loss: 0.0115\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0224 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0296 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0224 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0211 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0227 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0130\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0238 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0216 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0246 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0244 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0221 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0236 - val_loss: 0.0121\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0236 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0283 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0235 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0266 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0260 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0288 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0232 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0242 - val_loss: 0.0124\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0229 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0240 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0218 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0284 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0264 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0248 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0282 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0243 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0251 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0222 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0257 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0239 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0223 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 10ms/step - loss: 0.0233 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0300 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0269 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0283 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0237 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0135\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0267 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0234 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0279 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0256 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0298 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0317 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0255 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0221 - val_loss: 0.0135\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0233 - val_loss: 0.0122\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0239 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0247 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0238 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0229 - val_loss: 0.0132\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0302 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0260 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0226 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0217 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0274 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0270 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0212 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0219 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0310 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0278 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0141\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0251 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0240 - val_loss: 0.0137\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0247 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0265 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0283 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0259 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0219 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0254 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0308 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0263 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0147\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0126\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0269 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0284 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0232 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0238 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0221 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0244 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0145\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0250 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0295 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0287 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0215 - val_loss: 0.0137\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0250 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0234 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0226 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0261 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0235 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0222 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0232 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0142\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0285 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0274 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0240 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0232 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0286 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0131\n",
            "426/426 [==============================] - 7s 7ms/step - loss: 0.0222 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0209 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0221 - val_loss: 0.0150\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0221 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0316 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0212 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0143\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0245 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0261 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0223 - val_loss: 0.0141\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0212 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0237 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0249 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0301 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0141\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0288 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0124\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0223 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0243 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0261 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0231 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0308 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0282 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0273 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0221 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0245 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0138\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0259 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0235 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0281 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0298 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0259 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0233 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0229 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0261 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0140\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0264 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0218 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0245 - val_loss: 0.0124\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0235 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0317 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0278 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0121\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0231 - val_loss: 0.0133\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0225 - val_loss: 0.0140\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0249 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0220 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0231 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0271 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 10ms/step - loss: 0.0246 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0300 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0270 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0205 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0219 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0269 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0266 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0267 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0279 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0271 - val_loss: 0.0113\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0275 - val_loss: 0.0113\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0288 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0125\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0313 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0251 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0219 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0240 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0228 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0250 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0226 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0216 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0130\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0232 - val_loss: 0.0125\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0240 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0262 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0323 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0276 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0264 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0237 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0138\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0250 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 7ms/step - loss: 0.0256 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0233 - val_loss: 0.0154\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0231 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0293 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0263 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0231 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0144\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0306 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0222 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0245 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0252 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0145\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0215 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0317 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0248 - val_loss: 0.0121\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0238 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0224 - val_loss: 0.0139\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0268 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0242 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0299 - val_loss: 0.0113\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0236 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0217 - val_loss: 0.0132\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0217 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0280 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0250 - val_loss: 0.0125\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0221 - val_loss: 0.0126\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0214 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0264 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0251 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0214 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0256 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0116\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0272 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0241 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0247 - val_loss: 0.0127\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0284 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0138\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0241 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0273 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0211 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0235 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0217 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0142\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0256 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0230 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0216 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0281 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0226 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0239 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0283 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0244 - val_loss: 0.0126\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0232 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0141\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0210 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0287 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0291 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0331 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0335 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0217 - val_loss: 0.0128\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0242 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0239 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0287 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0302 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0272 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0143\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0257 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0250 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0141\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0223 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0213 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0119\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0239 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0320 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0139\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0250 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0230 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0130\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0250 - val_loss: 0.0124\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0257 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0235 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0236 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0303 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0235 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0235 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0217 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0230 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0231 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0244 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0238 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0273 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0294 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0229 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0218 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0247 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0304 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0213 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0220 - val_loss: 0.0143\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0217 - val_loss: 0.0138\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0294 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0220 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0266 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0246 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0271 - val_loss: 0.0119\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0245 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0142\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0264 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0240 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0288 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0280 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0282 - val_loss: 0.0119\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0257 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0227 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0250 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0268 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0255 - val_loss: 0.0117\n",
            "426/426 [==============================] - 6s 10ms/step - loss: 0.0257 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0243 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0223 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0226 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0243 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0267 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0276 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0120\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0243 - val_loss: 0.0126\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0255 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0214 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0276 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0303 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0292 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0248 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0268 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0264 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0271 - val_loss: 0.0115\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0246 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0274 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0267 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0274 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0270 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0225 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0240 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0231 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0235 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0287 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0220 - val_loss: 0.0134\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0227 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0279 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0239 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0269 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0246 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0241 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0254 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0232 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0271 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0125\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0248 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0230 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0309 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0262 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0214 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0235 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0235 - val_loss: 0.0123\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0222 - val_loss: 0.0137\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0265 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0254 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0266 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0212 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0211 - val_loss: 0.0126\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0304 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0268 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0248 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0261 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0299 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0225 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0227 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0218 - val_loss: 0.0147\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0222 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0365 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0219 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0264 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0246 - val_loss: 0.0127\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0246 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0249 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0226 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0262 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0261 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0282 - val_loss: 0.0115\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0215 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0271 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0309 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0231 - val_loss: 0.0124\n",
            "426/426 [==============================] - 7s 7ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0267 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0226 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0281 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0142\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0227 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0309 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0263 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0130\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0234 - val_loss: 0.0129\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0238 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0275 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0256 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0231 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0290 - val_loss: 0.0115\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0292 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0264 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0223 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0247 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0214 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0281 - val_loss: 0.0115\n",
            "426/426 [==============================] - 5s 9ms/step - loss: 0.0215 - val_loss: 0.0129\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0212 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0212 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0226 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0295 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0219 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0252 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0240 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0226 - val_loss: 0.0126\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0264 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0230 - val_loss: 0.0150\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0254 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0247 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0245 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0251 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0224 - val_loss: 0.0148\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0255 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0124\n",
            "426/426 [==============================] - 7s 7ms/step - loss: 0.0213 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0287 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0247 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0244 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0115\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0250 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0270 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0216 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0264 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0264 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0266 - val_loss: 0.0118\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0270 - val_loss: 0.0114\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0271 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0340 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0250 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 7ms/step - loss: 0.0309 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0236 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0269 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0273 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0115\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0233 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0241 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0206 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0278 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0256 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0241 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0285 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0219 - val_loss: 0.0132\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0215 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0255 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0254 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0264 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0237 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0230 - val_loss: 0.0132\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0220 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0244 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0215 - val_loss: 0.0139\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0220 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0257 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0240 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0122\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0269 - val_loss: 0.0120\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0224 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0258 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0267 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0259 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0270 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0216 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0246 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0277 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0261 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0218 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0225 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0213 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0226 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0116\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0283 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0233 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0220 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0217 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0271 - val_loss: 0.0119\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0264 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0267 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0246 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0238 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0249 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0149\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0222 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0313 - val_loss: 0.0112\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0231 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0225 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0307 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0284 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0239 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0248 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0263 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0242 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0307 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0254 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0146\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0265 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0324 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0214 - val_loss: 0.0140\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0264 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0274 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0230 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0283 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0230 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0258 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0271 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0228 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0270 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0225 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0257 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0222 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0252 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0314 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0208 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0268 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0231 - val_loss: 0.0130\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0240 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0246 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0237 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0295 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0146\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0269 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0227 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0256 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0153\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0283 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0222 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0235 - val_loss: 0.0129\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0304 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0276 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0270 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0260 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0295 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0215 - val_loss: 0.0141\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0246 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0240 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0243 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0250 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0243 - val_loss: 0.0122\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0297 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0276 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0262 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0249 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0264 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0252 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0122\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0219 - val_loss: 0.0134\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0245 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0205 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0275 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0247 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0293 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0291 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0233 - val_loss: 0.0133\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0245 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0141\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0221 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0228 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0228 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0205 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0254 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0222 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0300 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0257 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0242 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0141\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0355 - val_loss: 0.0113\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0219 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0231 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0149\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0218 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0236 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0122\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0283 - val_loss: 0.0116\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0230 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0279 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0246 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0250 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0243 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0222 - val_loss: 0.0145\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0140\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0226 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0288 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0216 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0242 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0254 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 5s 10ms/step - loss: 0.0297 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0249 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0281 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0235 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0229 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0243 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0285 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0235 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0123\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0230 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0218 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 7ms/step - loss: 0.0257 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0233 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0264 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0260 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0127\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0246 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0311 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0240 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0124\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0252 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0241 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0326 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0223 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0216 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0274 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0266 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0126\n",
            "426/426 [==============================] - 6s 10ms/step - loss: 0.0239 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0324 - val_loss: 0.0114\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0266 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0282 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0292 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0289 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0226 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0226 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0265 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0237 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0271 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0294 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0315 - val_loss: 0.0111\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0131\n",
            "426/426 [==============================] - 7s 4ms/step - loss: 0.0261 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0215 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0255 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0280 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0280 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0129\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0231 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0232 - val_loss: 0.0137\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0255 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0252 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0293 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0273 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0267 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0277 - val_loss: 0.0123\n",
            "426/426 [==============================] - 6s 10ms/step - loss: 0.0227 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0280 - val_loss: 0.0112\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0236 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0279 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0269 - val_loss: 0.0113\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0260 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0262 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0220 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0247 - val_loss: 0.0120\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0242 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0256 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0147\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0232 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0266 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0279 - val_loss: 0.0114\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0228 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0291 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0233 - val_loss: 0.0131\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0217 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0261 - val_loss: 0.0121\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0237 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0307 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0284 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0227 - val_loss: 0.0138\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0304 - val_loss: 0.0119\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0219 - val_loss: 0.0144\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0213 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0282 - val_loss: 0.0112\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0315 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0252 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0252 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0231 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0253 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0219 - val_loss: 0.0127\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0269 - val_loss: 0.0111\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0238 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0218 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0234 - val_loss: 0.0148\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0259 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0254 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0290 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0265 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0241 - val_loss: 0.0120\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0284 - val_loss: 0.0112\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0231 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0271 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0236 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0226 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0229 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0266 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0220 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0286 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0132\n",
            "426/426 [==============================] - 6s 10ms/step - loss: 0.0225 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0252 - val_loss: 0.0129\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0246 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0241 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0248 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0290 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0240 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0206 - val_loss: 0.0136\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0255 - val_loss: 0.0125\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0210 - val_loss: 0.0141\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0252 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0252 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0227 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0218 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0227 - val_loss: 0.0143\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0232 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0223 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0225 - val_loss: 0.0122\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0253 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0217 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0229 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0272 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0285 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0315 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0227 - val_loss: 0.0127\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0253 - val_loss: 0.0117\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0231 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0260 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0242 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0263 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0245 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0305 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0260 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0287 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0247 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0242 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0268 - val_loss: 0.0119\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0277 - val_loss: 0.0120\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0231 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0231 - val_loss: 0.0123\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0233 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0290 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0245 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0234 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0131\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0216 - val_loss: 0.0136\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0256 - val_loss: 0.0117\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0216 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0249 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0273 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0217 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0290 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 7ms/step - loss: 0.0253 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0143\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0257 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0227 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0279 - val_loss: 0.0115\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0232 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0252 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0229 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0272 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0221 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0273 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0141\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0237 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0224 - val_loss: 0.0130\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0228 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0226 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0250 - val_loss: 0.0115\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0215 - val_loss: 0.0131\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0223 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0248 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0220 - val_loss: 0.0145\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0281 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0222 - val_loss: 0.0127\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0284 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0247 - val_loss: 0.0120\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0223 - val_loss: 0.0125\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0258 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0236 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0224 - val_loss: 0.0131\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0208 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0216 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0250 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0311 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0291 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0214 - val_loss: 0.0131\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0260 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0213 - val_loss: 0.0135\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0254 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0299 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0232 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0321 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0236 - val_loss: 0.0118\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0140\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0132\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0126\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0245 - val_loss: 0.0128\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0252 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0219 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0223 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0214 - val_loss: 0.0129\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0250 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0288 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0253 - val_loss: 0.0122\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0231 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0241 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0282 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0259 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0124\n",
            "426/426 [==============================] - 5s 5ms/step - loss: 0.0218 - val_loss: 0.0125\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0224 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0315 - val_loss: 0.0115\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0265 - val_loss: 0.0113\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0225 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0219 - val_loss: 0.0147\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0224 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0242 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0267 - val_loss: 0.0123\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0275 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0254 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0215 - val_loss: 0.0133\n",
            "426/426 [==============================] - 6s 5ms/step - loss: 0.0330 - val_loss: 0.0118\n",
            "426/426 [==============================] - 5s 7ms/step - loss: 0.0240 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0233 - val_loss: 0.0132\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0274 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0239 - val_loss: 0.0119\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0130\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0247 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0222 - val_loss: 0.0135\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0256 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0227 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0232 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0223 - val_loss: 0.0139\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0228 - val_loss: 0.0139\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0230 - val_loss: 0.0133\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0224 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0248 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0218 - val_loss: 0.0148\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0236 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0246 - val_loss: 0.0117\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0229 - val_loss: 0.0136\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0259 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0122\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0251 - val_loss: 0.0120\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0228 - val_loss: 0.0144\n",
            "426/426 [==============================] - 3s 3ms/step - loss: 0.0311 - val_loss: 0.0114\n",
            "426/426 [==============================] - 5s 4ms/step - loss: 0.0241 - val_loss: 0.0124\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0270 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0260 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0121\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0238 - val_loss: 0.0118\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0207 - val_loss: 0.0126\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0244 - val_loss: 0.0117\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0220 - val_loss: 0.0139\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0230 - val_loss: 0.0133\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0238 - val_loss: 0.0129\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0225 - val_loss: 0.0137\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0261 - val_loss: 0.0117\n",
            "426/426 [==============================] - 7s 6ms/step - loss: 0.0249 - val_loss: 0.0119\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0244 - val_loss: 0.0128\n",
            "426/426 [==============================] - 4s 4ms/step - loss: 0.0256 - val_loss: 0.0127\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0299 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0248 - val_loss: 0.0121\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0231 - val_loss: 0.0124\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0234 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0218 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0222 - val_loss: 0.0136\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0258 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0231 - val_loss: 0.0134\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0270 - val_loss: 0.0114\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0245 - val_loss: 0.0125\n",
            "426/426 [==============================] - 7s 5ms/step - loss: 0.0288 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0226 - val_loss: 0.0134\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0242 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0239 - val_loss: 0.0126\n",
            "426/426 [==============================] - 4s 5ms/step - loss: 0.0295 - val_loss: 0.0112\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0317 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0128\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0292 - val_loss: 0.0115\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0265 - val_loss: 0.0116\n",
            "426/426 [==============================] - 4s 6ms/step - loss: 0.0298 - val_loss: 0.0116\n",
            "426/426 [==============================] - 3s 5ms/step - loss: 0.0233 - val_loss: 0.0125\n",
            "426/426 [==============================] - 3s 4ms/step - loss: 0.0249 - val_loss: 0.0115\n",
            "426/426 [==============================] - 6s 4ms/step - loss: 0.0252 - val_loss: 0.0116\n",
            "426/426 [==============================] - 5s 6ms/step - loss: 0.0298 - val_loss: 0.0112\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PSO Optimization\n",
        "lb = [1, 1]  # Lower bounds for epochs and batch_size\n",
        "ub = [1.5, 1.5]  # Upper bounds for epochs and batch_size\n",
        "\n",
        "# PSO optimization using the objective function and bounds\n",
        "best_params, _ = pso(objective_function, lb, ub, args=(x_train, y_train, x_test, y_test))\n",
        "\n",
        "# Use the best parameters to train the final LSTM model with early stopping\n",
        "best_epochs, best_batch_size = best_params\n",
        "final_lstm_model = create_lstm_model((x_train.shape[1], 1))\n",
        "final_lstm_model.fit(x_train, y_train, epochs=int(best_epochs), batch_size=int(best_batch_size), verbose=1,\n",
        "                     validation_data=(x_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa_2Lb6nMe_8"
      },
      "outputs": [],
      "source": [
        "# Function to create an MLP model\n",
        "def create_mlp_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_581n8QwMlOY",
        "outputId": "e9e8c9b2-373a-474a-b6f4-97056957fa3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 1s 2ms/step - loss: 0.2263\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1760\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1565\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1411\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1268\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1134\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.1008\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0882\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0766\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0660\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0563\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0479\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0406\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0344\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0296\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0257\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0231\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0209\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0198\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0186\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0180\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0175\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0176\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0176\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0168\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0172\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0174\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0172\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0176\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0169\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0173\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0174\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0172\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0170\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0173\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0171\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fca061b1c90>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train and evaluate the MLP model\n",
        "mlp_model = create_mlp_model(x_train.shape[1])\n",
        "mlp_model.fit(x_train, y_train, epochs=100, batch_size=8, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu0fLHknMqQQ",
        "outputId": "d23b35b1-713d-4d12-c4ff-60d932e6bb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "For x=10:\n",
            "LSTM Predicted y: 2142.9568712711334\n",
            "MLP Predicted y: 2142.9568712711334\n"
          ]
        }
      ],
      "source": [
        "# Example: Predict y for a specific x value using the trained models\n",
        "x_input = np.array([[10]])  # Replace with the desired value of x\n",
        "x_input_normalized = scaler_x.transform(x_input)\n",
        "y_lstm_pred_normalized = final_lstm_model.predict(x_input_normalized.reshape(1, 1, 1))[0, 0]\n",
        "y_lstm_pred = scaler_y.inverse_transform([[y_lstm_pred_normalized]])[0, 0]\n",
        "\n",
        "y_mlp_pred_normalized = mlp_model.predict(x_input_normalized)[0, 0]\n",
        "y_mlp_pred = scaler_y.inverse_transform([[y_mlp_pred_normalized]])[0, 0]\n",
        "\n",
        "print(f'For x={x_input[0, 0]}:')\n",
        "print(f'LSTM Predicted y: {y_lstm_pred}')\n",
        "print(f'MLP Predicted y: {y_mlp_pred}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}